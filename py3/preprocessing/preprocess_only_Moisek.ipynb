{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import parse_PAGE\n",
    "import cv2\n",
    "import line_extraction\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import codecs\n",
    "from svgpathtools import Path, Line\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6462894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_mapping(img, ts, path, offset_1, offset_2, max_min = None, cube_size = None):\n",
    "    # cube_size = 80\n",
    "\n",
    "    offset_1_pts = []\n",
    "    offset_2_pts = []\n",
    "    # for t in ts:\n",
    "    for i in range(len(ts)):\n",
    "        t = ts[i]\n",
    "        pt = path.point(t)\n",
    "\n",
    "        norm = None\n",
    "        if i == 0:\n",
    "            norm = normal(pt, path.point(ts[i+1]))\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        elif i == len(ts)-1:\n",
    "            norm = normal(path.point(ts[i-1]), pt)\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        else:\n",
    "            norm1 = normal(path.point(ts[i-1]), pt)\n",
    "            norm1 = norm1 / dis(complex(0,0), norm1)\n",
    "            norm2 = normal(pt, path.point(ts[i+1]))\n",
    "            norm2 = norm2 / dis(complex(0,0), norm2)\n",
    "\n",
    "            norm = (norm1 + norm2)/2\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "\n",
    "        offset_vector1 = offset_1 * norm\n",
    "        offset_vector2 = offset_2 * norm\n",
    "\n",
    "        pt1 = pt + offset_vector1\n",
    "        pt2 = pt + offset_vector2\n",
    "\n",
    "        offset_1_pts.append(complexToNpPt(pt1))\n",
    "        offset_2_pts.append(complexToNpPt(pt2))\n",
    "\n",
    "    offset_1_pts = np.array(offset_1_pts)\n",
    "    offset_2_pts = np.array(offset_2_pts)\n",
    "\n",
    "    h,w = img.shape[:2]\n",
    "\n",
    "    offset_source2 = np.array([(cube_size*i, 0) for i in range(len(offset_1_pts))], dtype=np.float32)\n",
    "    offset_source1 = np.array([(cube_size*i, cube_size) for i in range(len(offset_2_pts))], dtype=np.float32)\n",
    "\n",
    "    offset_source1 = offset_source1[::-1]\n",
    "    offset_source2 = offset_source2[::-1]\n",
    "\n",
    "    source = np.concatenate([offset_source1, offset_source2])\n",
    "    destination = np.concatenate([offset_1_pts, offset_2_pts])\n",
    "\n",
    "    source = source[:,::-1]\n",
    "    destination = destination[:,::-1]\n",
    "\n",
    "    n_w = int(offset_source2[:,0].max())\n",
    "    n_h = int(cube_size)\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:n_h, 0:n_w]\n",
    "\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    rectified_to_warped_x = map_x_32\n",
    "    rectified_to_warped_y = map_y_32\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:h, 0:w]\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(h,w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(h,w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    warped_to_rectified_x = map_x_32\n",
    "    warped_to_rectified_y = map_y_32\n",
    "\n",
    "    return rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min\n",
    "\n",
    "\n",
    "def dis(pt1, pt2):\n",
    "    a = (pt1.real - pt2.real)**2\n",
    "    b = (pt1.imag - pt2.imag)**2\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "def complexToNpPt(pt):\n",
    "    return np.array([pt.real, pt.imag], dtype=np.float32)\n",
    "\n",
    "def normal(pt1, pt2):\n",
    "    dif = pt1 - pt2\n",
    "    return complex(-dif.imag, dif.real)\n",
    "\n",
    "def find_t_spacing(path, cube_size):\n",
    "\n",
    "    l = path.length()\n",
    "    error = 0.01\n",
    "    init_step_size = cube_size / l\n",
    "\n",
    "    last_t = 0\n",
    "    cur_t = 0\n",
    "    pts = []\n",
    "    ts = [0]\n",
    "    pts.append(complexToNpPt(path.point(cur_t)))\n",
    "    path_lookup = {}\n",
    "    for target in np.arange(cube_size, int(l), cube_size):\n",
    "        step_size = init_step_size\n",
    "        for i in range(1000):\n",
    "            cur_length = dis(path.point(last_t), path.point(cur_t))\n",
    "            if np.abs(cur_length - cube_size) < error:\n",
    "                break\n",
    "\n",
    "            step_t = min(cur_t + step_size, 1.0)\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_t = max(cur_t - step_size, 0.0)\n",
    "            step_t = max(step_t, last_t)\n",
    "            step_t = max(step_t, 1.0)\n",
    "\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_size = step_size / 2.0\n",
    "\n",
    "        last_t = cur_t\n",
    "\n",
    "        ts.append(cur_t)\n",
    "        pts.append(complexToNpPt(path.point(cur_t)))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbd6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basename(img_name):\n",
    "    head, tail = os.path.split(img_name)\n",
    "    basename = 'base_' + tail\n",
    "    basename = basename[:-4]\n",
    "    return basename\n",
    "\n",
    "def handle_single_para(para_df, output_directory):\n",
    "    \n",
    "    output_data = []\n",
    "    num_lines = len(para_df)\n",
    "    print('....', num_lines)\n",
    "    if os.path.exists(para_df.image_file.iloc[0]):\n",
    "        img = cv2.imread(para_df.image_file.iloc[0])\n",
    "    else:\n",
    "        print('....File not found', para_df.image_file.iloc[0])\n",
    "        return ''\n",
    "    basename = get_basename(para_df.image_file.iloc[0])    \n",
    "    \n",
    "    all_lines = \"\"\n",
    "    \n",
    "    # get rid of png/jpg extension\n",
    "    \n",
    "    for region in [0]:\n",
    "        region_output_data = []\n",
    "        #print('in region', region)\n",
    "        for ind, line in enumerate(para_df.line_number):\n",
    "            if len(para_df.polygon_pts[ind]) == 0:\n",
    "                print('No polygon pts in img', para_df.image_file.iloc[0][-15:],\n",
    "                      'line number', line)\n",
    "                continue\n",
    "            #print('....ind, line', ind, line)\n",
    "            line_mask = line_extraction.extract_region_mask(img, para_df.polygon_pts[ind])\n",
    "            masked_img = img.copy()\n",
    "            masked_img[line_mask==0] = 0\n",
    "\n",
    "            summed_axis0 = (masked_img.astype(float) / 255).sum(axis=0)\n",
    "            summed_axis1 = (masked_img.astype(float) / 255).sum(axis=1)\n",
    "\n",
    "            non_zero_cnt0 = np.count_nonzero(summed_axis0) / float(len(summed_axis0))\n",
    "            non_zero_cnt1 = np.count_nonzero(summed_axis1) / float(len(summed_axis1))\n",
    "\n",
    "            avg_height0 = np.median(summed_axis0[summed_axis0 != 0])\n",
    "            avg_height1 = np.median(summed_axis1[summed_axis1 != 0])\n",
    "\n",
    "            avg_height = min(avg_height0, avg_height1)\n",
    "            if non_zero_cnt0 > non_zero_cnt1:\n",
    "                target_step_size = avg_height0\n",
    "            else:\n",
    "                target_step_size = avg_height1\n",
    "\n",
    "            paths = []\n",
    "            for i in range(len(para_df.baseline[ind])-1):\n",
    "                i_1 = i+1\n",
    "\n",
    "                p1 = para_df.baseline[ind][i]\n",
    "                p2 = para_df.baseline[ind][i_1]\n",
    "\n",
    "                p1_c = complex(*p1)\n",
    "                p2_c = complex(*p2)\n",
    "\n",
    "\n",
    "                paths.append(Line(p1_c, p2_c))\n",
    "\n",
    "\n",
    "            # Add a bit on the end\n",
    "            tan = paths[-1].unit_tangent(1.0)\n",
    "            p3_c = p2_c + target_step_size * tan\n",
    "            paths.append(Line(p2_c, p3_c))\n",
    "\n",
    "            path = Path(*paths)\n",
    "\n",
    "            ts = find_t_spacing(path, target_step_size)\n",
    "            \n",
    "            try:\n",
    "                #Changing this causes issues in pretraining - not sure why\n",
    "                target_height = 32\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 0, -2*target_step_size, cube_size = target_height)\n",
    "                warped_above = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 2*target_step_size, 0, cube_size = target_height)\n",
    "                warped_below = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                above_scale =  np.max((warped_above.astype(float) / 255).sum(axis=0))\n",
    "                below_scale = np.max((warped_below.astype(float) / 255).sum(axis=0))\n",
    "\n",
    "\n",
    "                \n",
    "                ab_sum = above_scale + below_scale\n",
    "                above = target_step_size * (above_scale/ab_sum)\n",
    "                below = target_step_size * (below_scale/ab_sum)\n",
    "\n",
    "                above = target_step_size * (above_scale/(target_height/2.0))\n",
    "                below = target_step_size * (below_scale/(target_height/2.0))\n",
    "                target_step_size = above + below\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                if len(ts) <= 1:\n",
    "                    print('Not doing line', line)\n",
    "                    continue\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, below, -above, cube_size=target_height)\n",
    "\n",
    "                ####MEHREEN COMMENT to prevent image from flipping\n",
    "                #rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "                #rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "                ###END MEHREEN COMMENT\n",
    "                warped_to_rectified_x = warped_to_rectified_x[::-1,::-1]\n",
    "                warped_to_rectified_y = warped_to_rectified_y[::-1,::-1]\n",
    "\n",
    "                warped = cv2.remap(img, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(255,255,255))\n",
    "\n",
    "                ####MEHREEN ADD\n",
    "                # Want to prevent image warping but we want coordinates to be warped\n",
    "                rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "                rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "\n",
    "                #### END MEHREEN ADD\n",
    "            except:\n",
    "                print('Not doing line', line)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            mapping = np.stack([rectified_to_warped_y, rectified_to_warped_x], axis=2)\n",
    "\n",
    "            top_left = mapping[0,0,:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "            btm_right = mapping[min(mapping.shape[0]-1, target_height-1), min(mapping.shape[1]-1, target_height-1),:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "\n",
    "\n",
    "            line_points = []\n",
    "            for i in range(0,mapping.shape[1],target_height):\n",
    "\n",
    "                x0 = float(rectified_to_warped_x[0,i])\n",
    "                x1 = float(rectified_to_warped_x[-1,i])\n",
    "\n",
    "                y0 = float(rectified_to_warped_y[0,i])\n",
    "                y1 = float(rectified_to_warped_y[-1,i])\n",
    "\n",
    "                line_points.append({\n",
    "                    \"x0\": x0, #MEhreen change x0,\n",
    "                    \"x1\": x1, #Mehreen change x1,\n",
    "                    \"y0\": y1, #Mehreen change from y0,\n",
    "                    \"y1\": y0, #Mehreen change from y1\n",
    "                })\n",
    "                \n",
    "                \n",
    "                                \n",
    "            ###Mehreen add for viewing\n",
    "\n",
    "#            plt.imshow(img) # or display line warped\n",
    "#            print(\"****\", line_points)\n",
    "#            for coord in line_points:\n",
    "#                x = coord[\"x0\"]\n",
    "#                y = coord[\"y0\"]\n",
    "#                x1 = coord[\"x1\"]\n",
    "#                y1 = coord[\"y1\"]\n",
    "                #rect = patches.Rectangle((x, y), np.abs(x-coord[2]), np.abs(y-coord[3]), facecolor='green')\n",
    "#                rect = patches.Rectangle((x, y), 10, 10, facecolor='blue')\n",
    "#                rect1 = patches.Rectangle((x1, y1), 10, 10, facecolor='red')\n",
    "#                plt.gca().add_patch(rect)  \n",
    "#                plt.gca().add_patch(rect1)\n",
    "#            rect0 = patches.Rectangle((line_points[0][\"x0\"], line_points[0][\"y0\"]), 10, 10, facecolor='yellow') \n",
    "#            plt.gca().add_patch(rect0)\n",
    "#            plt.show()\n",
    "             ## ENd mehreen add for view   \n",
    "                \n",
    "            \n",
    "            output_file = os.path.join(output_directory, \n",
    "                          basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            warp_output_file = os.path.join(output_directory, basename, \"{}-{}.png\".format(basename, line))\n",
    "            warp_output_file_save = os.path.join(basename, \"{}-{}.png\".format(basename, str(len(region_output_data))))\n",
    "            save_file = os.path.join(basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            region_output_data.append({\n",
    "                \"gt\": para_df.ground_truth[ind],\n",
    "                \"image_path\": save_file,\n",
    "                \"sol\": line_points[0],\n",
    "                \"lf\": line_points,\n",
    "                \"hw_path\": warp_output_file #MEhreen commentwarp_output_file_save\n",
    "            })\n",
    "            #print('****', output_file)\n",
    "            if not os.path.exists(os.path.dirname(output_file)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(output_file))\n",
    "                except OSError as exc:\n",
    "                    raise Exception(\"Could not write file\")\n",
    "\n",
    "            cv2.imwrite(warp_output_file, warped)\n",
    "\n",
    "        output_data.extend(region_output_data)\n",
    "\n",
    "    \n",
    "    if len(region_output_data) == 0:\n",
    "        return ''\n",
    "        \n",
    "    output_data_path =os.path.join(output_directory, basename, \"{}.json\".format(basename))\n",
    "    if not os.path.exists(os.path.dirname(output_data_path)):\n",
    "        os.makedirs(os.path.dirname(output_data_path))\n",
    "\n",
    "    with open(output_data_path, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "\n",
    "    return output_data_path    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3700d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_int_tuples(df_col):\n",
    "    tmp_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        if not pd.isna(item):\n",
    "            item = eval(item)    \n",
    "            tmp_col.append([(round(x[0]), round(x[1])) for x in item])\n",
    "        else:\n",
    "            tmp_col.append([])\n",
    "    return tmp_col\n",
    "\n",
    "def rotate_polygon(p):\n",
    "    if len(p) == 8 or len(p) == 7:\n",
    "        poly = p[4:]\n",
    "        poly.extend(p[0:4])\n",
    "        return poly\n",
    "    if len(p) == 4:\n",
    "        poly = [p[2], p[3], p[0], p[1]]\n",
    "        return poly\n",
    "    else:\n",
    "        print(\"something wrong\", p)\n",
    "        return []\n",
    "    \n",
    "def rotate_poly_list(df_col):\n",
    "    poly_list = [rotate_polygon(p) for p in df_col]\n",
    "    return poly_list\n",
    "\n",
    "\n",
    "def rotate_baseline_list(df_col):\n",
    "    b_list = [b[::-1] for b in df_col]\n",
    "    return b_list\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374673a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in set all_files_03: 65\n",
      "....Doing base_AR53_02_205\n",
      ".... 26\n",
      "....Doing base_AR53_02_207\n",
      ".... 26\n",
      "....Doing base_AR53_02_209\n",
      ".... 25\n",
      "....Doing base_AR53_02_210\n",
      ".... 24\n",
      "....Doing base_AR53_02_211\n",
      ".... 24\n",
      "....Doing base_AR53_02_212\n",
      ".... 24\n",
      "....Doing base_AR53_02_213\n",
      ".... 23\n",
      "....Doing base_AR53_02_230\n",
      ".... 26\n",
      "....Doing base_AR53_02_246\n",
      ".... 26\n",
      "....Doing base_AR53_02_254\n",
      ".... 24\n",
      "....Doing base_AR53_02_262\n",
      ".... 26\n",
      "....Doing base_KJoseph19020905-002\n",
      ".... 30\n",
      "....Doing base_KJoseph19030207-001\n",
      ".... 20\n",
      "....Doing base_KJoseph19040209-001\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-001\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-002\n",
      ".... 24\n",
      "....Doing base_KJoseph19040819-001\n",
      ".... 20\n",
      "....Doing base_KJoseph19040819-002\n",
      ".... 26\n",
      "....Doing base_KJoseph19040819-003\n",
      ".... 21\n",
      "....Doing base_KJoseph19040819-004\n",
      ".... 27\n",
      "....Doing base_KJoseph19050928-002\n",
      ".... 27\n",
      "....Doing base_KJoseph19080319-001\n",
      ".... 21\n",
      "....Doing base_Nasrallah-Al001-034b\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034d\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034e\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034f\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034h\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034i\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034j\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034k\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034l\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034m\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034n\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034p\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034q\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-035a\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-035b\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-035c\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046a\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046b\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-077\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-103\n",
      ".... 28\n",
      "....Doing base_TAttallah2020-107\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-122a\n",
      ".... 31\n",
      "....Doing base_TAttallah2020-125a\n",
      ".... 25\n",
      "....Doing base_TAttallah2020-128c\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-130\n",
      ".... 23\n",
      "....Doing base_TAttallah2020-132a\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-132b\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-134\n",
      ".... 26\n",
      "....Doing base_kc0066_01_01_1873diary_061\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_062\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_063\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_064\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_065\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_164\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_165\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_166\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_167\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_168\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_169\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_170\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_171\n",
      ".... 13\n"
     ]
    }
   ],
   "source": [
    "def process_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    \n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df.reset_index(inplace=True)\n",
    "        valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        head, tail = os.path.split(para_df.image_file.iloc[0])\n",
    "        basename = 'base_' + tail\n",
    "        basename = basename[:-4]\n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        \n",
    "        json_path = handle_single_para(para_df, out_path+set_name)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "\n",
    "def process_notdone_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    \n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    \n",
    "    \n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "\n",
    "        valid_region = False\n",
    "        if not pd.isna(para_df['region_type'][0]):\n",
    "            valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        \n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        basename = get_basename(img_path)\n",
    "        \n",
    "        tmp_path = os.path.join(out_path, set_name, basename, basename + '.json')\n",
    "        \n",
    "        #print('...', tmp_path)\n",
    "        if os.path.isfile(tmp_path):\n",
    "            json_path = tmp_path\n",
    "            print('... Done', basename)\n",
    "        else: \n",
    "            print('....Doing', basename)\n",
    "            json_path = handle_single_para(para_df, out_path+set_name)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            print('Not added', basename)\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "def main_arabic_preprocess():\n",
    "    \n",
    "    # Make sure there is a / at end of path\n",
    "    paths = [\n",
    "             '/home/msaeed3/mehreen/datasets/MoiseK/datasets/', \n",
    "             ]\n",
    "    # Previously For batch_01\n",
    "    #set_name = ['Train_new', 'Valid_new', 'Test_new']\n",
    "    # Now changed to Train_01, Valid_01, Test_01\n",
    "    # For batch_01 corresponding file folder is all_files\n",
    "    # For batch_02 corresponding file folder is all_files_02\n",
    "    # For batch_03\n",
    "    #set_name = ['Train_03', 'Valid_03', 'Test_03']\n",
    "    set_name = ['all_files_03']\n",
    "\n",
    "    all_train = []\n",
    "    all_valid = []\n",
    "    all_test = []\n",
    "    for path in paths:\n",
    "        set_to_use = set_name\n",
    "        train_gt = process_notdone_arabic_dir(path, 'all_files_03', ['paragraph', 'text'], \n",
    "                                      input_file= os.path.join(path, set_to_use[0] +'.csv'))\n",
    "        #valid_gt = process_notdone_arabic_dir(path, 'all_files_03', ['paragraph', 'text'],\n",
    "        #                              input_file= os.path.join(path, set_to_use[1] +'.csv'))\n",
    "        #test_gt = process_notdone_arabic_dir(path, 'all_files_03', ['paragraph', 'text'],\n",
    "        #                              input_file= os.path.join(path, set_to_use[2] +'.csv'))\n",
    "\n",
    "        all_train.extend(train_gt)\n",
    "        #all_valid.extend(valid_gt)\n",
    "        #all_test.extend(test_gt)\n",
    "        \n",
    "    path = '/home/msaeed3/mehreen/datasets/MoiseK/datasets/'     \n",
    "    train_json = os.path.join(path, set_name[0]+'.json')\n",
    "    #valid_json = os.path.join(path, set_name[1]+'.json')\n",
    "    #test_json = os.path.join(path, set_name[2] + '.json')\n",
    "    with open(train_json, 'w') as f:\n",
    "        json.dump(all_train, f)\n",
    "\n",
    "    #with open(valid_json, 'w') as f:\n",
    "    #    json.dump(all_valid, f)    \n",
    "    \n",
    "    #with open(test_json, 'w') as f:\n",
    "    #    json.dump(all_test, f)    \n",
    "        \n",
    "main_arabic_preprocess()\n",
    "#print('Preprocessing done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797109ed",
   "metadata": {},
   "source": [
    "# Uncomment this cell to check for a single file\n",
    "\n",
    "out_path = '/home/msaeed3/mehreen/datasets/MoiseK/'\n",
    "\n",
    "check_file = 'AR69_004.jpg' \n",
    "set_name = 'Valid'\n",
    "\n",
    "df = pd.read_csv(os.path.join(out_path, set_name + '.csv'))\n",
    "df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "#df.baseline = rotate_baseline_list(df.baseline)\n",
    "df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "#df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "\n",
    "files = df.image_file\n",
    "\n",
    "all_ground_truth = []\n",
    "for img_path in set(files):\n",
    "    \n",
    "    if check_file in img_path:\n",
    "        print('***')\n",
    "        para_df = df[df.image_file == img_path]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        valid_region = any(s in para_df['region_type'][0] for s in ['paragraph','text'])\n",
    "        if not valid_region:\n",
    "            print('NOT DONE')\n",
    "        json_path = handle_single_para(para_df, out_path+set_name)       \n",
    "        print(json_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16415cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpara_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'para_df' is not defined"
     ]
    }
   ],
   "source": [
    "para_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for priinting the json file in readable format\n",
    "\n",
    "json_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train.json'\n",
    "with open(json_file, 'r') as fin:\n",
    "    json_obj = json.load(fin)\n",
    "\n",
    "out_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train_readable.json'\n",
    "with open(out_file, 'w') as fout:\n",
    "    json.dump(json_obj, fout, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
