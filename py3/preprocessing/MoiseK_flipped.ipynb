{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import parse_PAGE\n",
    "import cv2\n",
    "import line_extraction\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import codecs\n",
    "from svgpathtools import Path, Line\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "HT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6462894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_mapping(img, ts, path, offset_1, offset_2, max_min = None, cube_size = None):\n",
    "    # cube_size = 80\n",
    "\n",
    "    offset_1_pts = []\n",
    "    offset_2_pts = []\n",
    "    # for t in ts:\n",
    "    for i in range(len(ts)):\n",
    "        t = ts[i]\n",
    "        pt = path.point(t)\n",
    "\n",
    "        norm = None\n",
    "        if i == 0:\n",
    "            norm = normal(pt, path.point(ts[i+1]))\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        elif i == len(ts)-1:\n",
    "            norm = normal(path.point(ts[i-1]), pt)\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        else:\n",
    "            norm1 = normal(path.point(ts[i-1]), pt)\n",
    "            norm1 = norm1 / dis(complex(0,0), norm1)\n",
    "            norm2 = normal(pt, path.point(ts[i+1]))\n",
    "            norm2 = norm2 / dis(complex(0,0), norm2)\n",
    "\n",
    "            norm = (norm1 + norm2)/2\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "\n",
    "        offset_vector1 = offset_1 * norm\n",
    "        offset_vector2 = offset_2 * norm\n",
    "\n",
    "        pt1 = pt + offset_vector1\n",
    "        pt2 = pt + offset_vector2\n",
    "\n",
    "        offset_1_pts.append(complexToNpPt(pt1))\n",
    "        offset_2_pts.append(complexToNpPt(pt2))\n",
    "\n",
    "    offset_1_pts = np.array(offset_1_pts)\n",
    "    offset_2_pts = np.array(offset_2_pts)\n",
    "\n",
    "    h,w = img.shape[:2]\n",
    "\n",
    "    offset_source2 = np.array([(cube_size*i, 0) for i in range(len(offset_1_pts))], dtype=np.float32)\n",
    "    offset_source1 = np.array([(cube_size*i, cube_size) for i in range(len(offset_2_pts))], dtype=np.float32)\n",
    "\n",
    "    offset_source1 = offset_source1[::-1]\n",
    "    offset_source2 = offset_source2[::-1]\n",
    "\n",
    "    source = np.concatenate([offset_source1, offset_source2])\n",
    "    destination = np.concatenate([offset_1_pts, offset_2_pts])\n",
    "\n",
    "    source = source[:,::-1]\n",
    "    destination = destination[:,::-1]\n",
    "\n",
    "    n_w = int(offset_source2[:,0].max())\n",
    "    n_h = int(cube_size)\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:n_h, 0:n_w]\n",
    "\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    rectified_to_warped_x = map_x_32\n",
    "    rectified_to_warped_y = map_y_32\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:h, 0:w]\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(h,w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(h,w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    warped_to_rectified_x = map_x_32\n",
    "    warped_to_rectified_y = map_y_32\n",
    "\n",
    "    return rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min\n",
    "\n",
    "\n",
    "def dis(pt1, pt2):\n",
    "    a = (pt1.real - pt2.real)**2\n",
    "    b = (pt1.imag - pt2.imag)**2\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "def complexToNpPt(pt):\n",
    "    return np.array([pt.real, pt.imag], dtype=np.float32)\n",
    "\n",
    "def normal(pt1, pt2):\n",
    "    dif = pt1 - pt2\n",
    "    return complex(-dif.imag, dif.real)\n",
    "\n",
    "def find_t_spacing(path, cube_size):\n",
    "\n",
    "    l = path.length()\n",
    "    error = 0.01\n",
    "    init_step_size = cube_size / l\n",
    "\n",
    "    last_t = 0\n",
    "    cur_t = 0\n",
    "    pts = []\n",
    "    ts = [0]\n",
    "    pts.append(complexToNpPt(path.point(cur_t)))\n",
    "    path_lookup = {}\n",
    "    for target in np.arange(cube_size, int(l), cube_size):\n",
    "        step_size = init_step_size\n",
    "        for i in range(1000):\n",
    "            cur_length = dis(path.point(last_t), path.point(cur_t))\n",
    "            if np.abs(cur_length - cube_size) < error:\n",
    "                break\n",
    "\n",
    "            step_t = min(cur_t + step_size, 1.0)\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_t = max(cur_t - step_size, 0.0)\n",
    "            step_t = max(step_t, last_t)\n",
    "            step_t = max(step_t, 1.0)\n",
    "\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_size = step_size / 2.0\n",
    "\n",
    "        last_t = cur_t\n",
    "\n",
    "        ts.append(cur_t)\n",
    "        pts.append(complexToNpPt(path.point(cur_t)))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    return ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbd6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basename(img_name):\n",
    "    head, tail = os.path.split(img_name)\n",
    "    basename = 'base_' + tail\n",
    "    basename = basename[:-4]\n",
    "    return basename\n",
    "\n",
    "def handle_single_para(para_df, output_directory, flip=False):\n",
    "    \n",
    "    output_data = []\n",
    "    num_lines = len(para_df)\n",
    "    print('....', num_lines)\n",
    "    if os.path.exists(para_df.image_file.iloc[0]):\n",
    "        img = cv2.imread(para_df.image_file.iloc[0])\n",
    "        if flip:\n",
    "            img = cv2.flip(img, 1)\n",
    "    else:\n",
    "        print('....File not found', para_df.image_file.iloc[0])\n",
    "        return ''\n",
    "    basename = get_basename(para_df.image_file.iloc[0])    \n",
    "    \n",
    "    all_lines = \"\"\n",
    "    \n",
    "    # get rid of png/jpg extension\n",
    "    \n",
    "    for region in [0]:\n",
    "        region_output_data = []\n",
    "        #print('in region', region)\n",
    "        for ind, line in enumerate(para_df.line_number):\n",
    "            if len(para_df.polygon_pts[ind]) == 0:\n",
    "                print('No polygon pts in img', para_df.image_file.iloc[0][-15:],\n",
    "                      'line number', line)\n",
    "                continue\n",
    "            #print('....ind, line', ind, line)\n",
    "            line_mask = line_extraction.extract_region_mask(img, para_df.polygon_pts[ind])\n",
    "            masked_img = img.copy()\n",
    "            masked_img[line_mask==0] = 0\n",
    "\n",
    "            summed_axis0 = (masked_img.astype(float) / 255).sum(axis=0)\n",
    "            summed_axis1 = (masked_img.astype(float) / 255).sum(axis=1)\n",
    "\n",
    "            non_zero_cnt0 = np.count_nonzero(summed_axis0) / float(len(summed_axis0))\n",
    "            non_zero_cnt1 = np.count_nonzero(summed_axis1) / float(len(summed_axis1))\n",
    "\n",
    "            avg_height0 = np.median(summed_axis0[summed_axis0 != 0])\n",
    "            avg_height1 = np.median(summed_axis1[summed_axis1 != 0])\n",
    "\n",
    "            avg_height = min(avg_height0, avg_height1)\n",
    "            if non_zero_cnt0 > non_zero_cnt1:\n",
    "                target_step_size = avg_height0\n",
    "            else:\n",
    "                target_step_size = avg_height1\n",
    "\n",
    "            paths = []\n",
    "            for i in range(len(para_df.baseline[ind])-1):\n",
    "                i_1 = i+1\n",
    "\n",
    "                p1 = para_df.baseline[ind][i]\n",
    "                p2 = para_df.baseline[ind][i_1]\n",
    "\n",
    "                p1_c = complex(*p1)\n",
    "                p2_c = complex(*p2)\n",
    "\n",
    "\n",
    "                paths.append(Line(p1_c, p2_c))\n",
    "\n",
    "\n",
    "            if len(paths) == 0:\n",
    "                print('Path length is 0', 'for ind', ind)\n",
    "                continue\n",
    "            # Add a bit on the end\n",
    "            tan = paths[-1].unit_tangent(1.0)\n",
    "            p3_c = p2_c + target_step_size * tan\n",
    "            paths.append(Line(p2_c, p3_c))\n",
    "\n",
    "            path = Path(*paths)\n",
    "            \n",
    "            try:\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                \n",
    "                #Changing this causes issues in pretraining - not sure why\n",
    "                target_height = HT\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 0, -2*target_step_size, cube_size = target_height)\n",
    "                warped_above = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 2*target_step_size, 0, cube_size = target_height)\n",
    "                warped_below = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                above_scale =  np.max((warped_above.astype(float) / 255).sum(axis=0))\n",
    "                below_scale = np.max((warped_below.astype(float) / 255).sum(axis=0))\n",
    "\n",
    "\n",
    "                \n",
    "                ab_sum = above_scale + below_scale\n",
    "                above = target_step_size * (above_scale/ab_sum)\n",
    "                below = target_step_size * (below_scale/ab_sum)\n",
    "\n",
    "                above = target_step_size * (above_scale/(target_height/2.0))\n",
    "                below = target_step_size * (below_scale/(target_height/2.0))\n",
    "                target_step_size = above + below\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                if len(ts) <= 1:\n",
    "                    print('Not doing line', line)\n",
    "                    continue\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, below, -above, cube_size=target_height)\n",
    "\n",
    "                rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "                rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "\n",
    "                warped_to_rectified_x = warped_to_rectified_x[::-1,::-1]\n",
    "                warped_to_rectified_y = warped_to_rectified_y[::-1,::-1]\n",
    "\n",
    "                warped = cv2.remap(img, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(255,255,255))\n",
    "            except:\n",
    "                print('Not doing line', line)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            mapping = np.stack([rectified_to_warped_y, rectified_to_warped_x], axis=2)\n",
    "\n",
    "            top_left = mapping[0,0,:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "            btm_right = mapping[min(mapping.shape[0]-1, target_height-1), min(mapping.shape[1]-1, target_height-1),:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "\n",
    "\n",
    "            line_points = []\n",
    "            for i in range(0,mapping.shape[1],target_height):\n",
    "\n",
    "                x0 = float(rectified_to_warped_x[0,i])\n",
    "                x1 = float(rectified_to_warped_x[-1,i])\n",
    "\n",
    "                y0 = float(rectified_to_warped_y[0,i])\n",
    "                y1 = float(rectified_to_warped_y[-1,i])\n",
    "\n",
    "                line_points.append({\n",
    "                    \"x0\": x0, \n",
    "                    \"x1\": x1, \n",
    "                    \"y0\": y0, \n",
    "                    \"y1\": y1, \n",
    "                })\n",
    "                \n",
    "                \n",
    "                                \n",
    "            ###Mehreen add for viewing\n",
    "\n",
    "#            plt.imshow(img) # or display line warped\n",
    "#            print(\"****\", line_points)\n",
    "#            for coord in line_points:\n",
    "#                x = coord[\"x0\"]\n",
    "#                y = coord[\"y0\"]\n",
    "#                x1 = coord[\"x1\"]\n",
    "#                y1 = coord[\"y1\"]\n",
    "                #rect = patches.Rectangle((x, y), np.abs(x-coord[2]), np.abs(y-coord[3]), facecolor='green')\n",
    "#                rect = patches.Rectangle((x, y), 10, 10, facecolor='blue')\n",
    "#                rect1 = patches.Rectangle((x1, y1), 10, 10, facecolor='red')\n",
    "#                plt.gca().add_patch(rect)  \n",
    "#                plt.gca().add_patch(rect1)\n",
    "#            rect0 = patches.Rectangle((line_points[0][\"x0\"], line_points[0][\"y0\"]), 10, 10, facecolor='yellow') \n",
    "#            plt.gca().add_patch(rect0)\n",
    "#            plt.show()\n",
    "             ## ENd mehreen add for view   \n",
    "                \n",
    "            \n",
    "            output_file = os.path.join(output_directory, \n",
    "                          basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            warp_output_file = os.path.join(output_directory, basename, \"{}-{}.png\".format(basename, line))\n",
    "            warp_output_file_save = os.path.join(basename, \"{}-{}.png\".format(basename, str(len(region_output_data))))\n",
    "            save_file = os.path.join(basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            region_output_data.append({\n",
    "                \"gt\": para_df.ground_truth[ind],\n",
    "                \"image_path\": save_file,\n",
    "                \"sol\": line_points[0],\n",
    "                \"lf\": line_points,\n",
    "                \"hw_path\": warp_output_file #MEhreen commentwarp_output_file_save\n",
    "            })\n",
    "            #print('****', output_file)\n",
    "            if not os.path.exists(os.path.dirname(output_file)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(output_file))\n",
    "                except OSError as exc:\n",
    "                    raise Exception(\"Could not write file\")\n",
    "\n",
    "            cv2.imwrite(warp_output_file, warped)\n",
    "\n",
    "        output_data.extend(region_output_data)\n",
    "\n",
    "    \n",
    "    if len(region_output_data) == 0:\n",
    "        return ''\n",
    "        \n",
    "    output_data_path =os.path.join(output_directory, basename, \"{}.json\".format(basename))\n",
    "    if not os.path.exists(os.path.dirname(output_data_path)):\n",
    "        os.makedirs(os.path.dirname(output_data_path))\n",
    "\n",
    "    with open(output_data_path, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "\n",
    "    return output_data_path    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3700d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_int_tuples(df_col):\n",
    "    tmp_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        if not pd.isna(item):\n",
    "            item = eval(item)    \n",
    "            tmp_col.append([(round(x[0]), round(x[1])) for x in item])\n",
    "        else:\n",
    "            tmp_col.append([])\n",
    "    return tmp_col\n",
    "\n",
    "def rotate_polygon(p):\n",
    "    if len(p) == 8 or len(p) == 7:\n",
    "        poly = p[4:]\n",
    "        poly.extend(p[0:4])\n",
    "        return poly\n",
    "    if len(p) == 4:\n",
    "        poly = [p[2], p[3], p[0], p[1]]\n",
    "        return poly\n",
    "    else:\n",
    "        print(\"something wrong\", p)\n",
    "        return []\n",
    "    \n",
    "def rotate_poly_list(df_col):\n",
    "    poly_list = [rotate_polygon(p) for p in df_col]\n",
    "    return poly_list\n",
    "\n",
    "\n",
    "def rotate_baseline_list(df_col):\n",
    "    b_list = [b[::-1] for b in df_col]\n",
    "    return b_list\n",
    "    \n",
    "# Reverse is true for right to left reading order    \n",
    "def remove_duplicate_baseline(baseline, reverse):\n",
    "    baseline.sort(key=lambda x: x[0], reverse=reverse)\n",
    "    unique_pts = [baseline[0]]\n",
    "    for pt in baseline[1:]:\n",
    "        if pt != unique_pts[-1]:\n",
    "            unique_pts.append(pt)\n",
    "    \n",
    "    return unique_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374673a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_img_dim(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    ht, width = img.shape[:2]\n",
    "    return (ht, width)\n",
    "\n",
    "# Subtract all x-coord from img_width as for Arabic we flip the image horizontally \n",
    "# Right to left reading order\n",
    "# Set reverse=True for baseline so that reading order becomes left to righ\n",
    "def flip_x_coord(df_col, img_width, reverse=False):\n",
    "    new_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        flipped = [(img_width-x, y) for (x,y) in item]\n",
    "        if reverse:\n",
    "            flipped = flipped[::-1]\n",
    "        new_col.append(flipped)\n",
    "    return new_col\n",
    "\n",
    "def process_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    DO_NOT_USE_WITHOUT_FLIPPING\n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df.reset_index(inplace=True)\n",
    "        valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        head, tail = os.path.split(para_df.image_file.iloc[0])\n",
    "        basename = 'base_' + tail\n",
    "        basename = basename[:-4]\n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        \n",
    "        json_path = handle_single_para(para_df, out_path+set_name)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "\n",
    "def process_notdone_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    \n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    \n",
    "    \n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        \n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        \n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.baseline = [remove_duplicate_baseline(b, reverse=False) for b in para_df.baseline]\n",
    "        \n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "\n",
    "        valid_region = False\n",
    "        if not pd.isna(para_df['region_type'][0]):\n",
    "            valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        \n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        basename = get_basename(img_path)\n",
    "        \n",
    "        tmp_path = os.path.join(out_path, set_name, basename, basename + '.json')\n",
    "        \n",
    "        #print('...', tmp_path)\n",
    "        if os.path.isfile(tmp_path):\n",
    "            json_path = tmp_path\n",
    "            print('... Done', basename)\n",
    "        else: \n",
    "            print('....Doing', basename)\n",
    "            json_path = handle_single_para(para_df, out_path+set_name, flip=True)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            print('Not added', basename)\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "def main_arabic_preprocess():\n",
    "    \n",
    "    # Make sure there is a / at end of path\n",
    "    paths = [\n",
    "             '/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr/', \n",
    "             ]\n",
    "    # Previously For batch_01\n",
    "    #set_name = ['Train_new', 'Valid_new', 'Test_new']\n",
    "    # Now changed to Train_01, Valid_01, Test_01\n",
    "    # For batch_01 corresponding file folder is all_files\n",
    "    # For batch_02 corresponding file folder is all_files_02\n",
    "    # For batch_03\n",
    "    #set_name = ['all_01', 'all_02', 'all_03']\n",
    "    set_name = ['all_03']\n",
    "\n",
    "    all_train = []\n",
    "    all_valid = []\n",
    "    all_test = []\n",
    "    for set_to_use in set_name:\n",
    "        \n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file= os.path.join(paths[0], set_to_use +'.csv'))\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "\n",
    " \n",
    "# This routine is for MoiseK batch01, batch02 and batch03\n",
    "#main_arabic_preprocess()\n",
    "#print('Preprocessing done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac21dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in set all_07: 214\n",
      "....Doing base_00000000001(72)_Page_2\n",
      ".... 35\n",
      "....Doing base_00000000001(72)_Page_3\n",
      ".... 33\n",
      "....Doing base_00000000001(72)_Page_4\n",
      ".... 32\n",
      "....Doing base_000000000010(71)_Page_1\n",
      ".... 17\n",
      "....Doing base_000000000010_(100)\n",
      ".... 25\n",
      "....Doing base_000000000010_(101)\n",
      ".... 29\n",
      "....Doing base_000000000010_(102)_Page_1\n",
      ".... 21\n",
      "....Doing base_000000000010_(117)_Page_1\n",
      ".... 25\n",
      "....Doing base_000000000010_(12)\n",
      ".... 22\n",
      "....Doing base_000000000010_(125)_Page_1\n",
      ".... 14\n",
      "....Doing base_000000000010_(135)_Page_1\n",
      ".... 44\n",
      "....Doing base_000000000010_(135)_Page_2\n",
      ".... 41\n",
      "....Doing base_000000000010_(135)_Page_3\n",
      ".... 34\n",
      "....Doing base_000000000010_(139)_Page_1\n",
      ".... 23\n",
      "....Doing base_000000000010_(139)_Page_2\n",
      ".... 26\n",
      "....Doing base_000000000010_(139)_Page_2a\n",
      ".... 24\n",
      "....Doing base_000000000010_(139)_Page_3\n",
      ".... 29\n",
      "....Doing base_000000000010_(139)_Page_3a\n",
      ".... 28\n",
      "....Doing base_000000000010_(140)_Page_1\n",
      ".... 23\n",
      "....Doing base_000000000010_(20)_Page_1\n",
      ".... 15\n",
      "....Doing base_000000000010_(23)\n",
      ".... 15\n",
      "....Doing base_000000000010_(33)_Page_1\n",
      ".... 19\n",
      "....Doing base_000000000010_(33)_Page_3\n",
      ".... 17\n",
      "....Doing base_000000000010_(4)_Page_1\n",
      ".... 35\n",
      "....Doing base_000000000010_(4)_Page_2\n",
      ".... 32\n",
      "Not doing line 32\n",
      "....Doing base_000000000010_(4)_Page_3\n",
      ".... 31\n",
      "....Doing base_000000000010_(40)\n",
      ".... 21\n",
      "....Doing base_000000000010_(41)_Page_02a\n",
      ".... 21\n",
      "....Doing base_000000000010_(41)_Page_02b\n",
      ".... 19\n",
      "....Doing base_000000000010_(41)_Page_03a\n",
      ".... 21\n",
      "....Doing base_000000000010_(41)_Page_03b\n",
      ".... 20\n",
      "....Doing base_000000000010_(41)_Page_04a\n",
      ".... 23\n",
      "....Doing base_000000000010_(41)_Page_04b\n",
      ".... 20\n",
      "....Doing base_000000000010_(41)_Page_05a\n",
      ".... 19\n",
      "....Doing base_000000000010_(41)_Page_05b\n",
      ".... 20\n",
      "....Doing base_000000000010_(41)_Page_06a\n",
      ".... 23\n",
      "....Doing base_000000000010_(41)_Page_06b\n",
      ".... 20\n",
      "Not doing line 20\n",
      "....Doing base_000000000010_(41)_Page_07a\n",
      ".... 22\n",
      "....Doing base_000000000010_(41)_Page_07b\n",
      ".... 21\n",
      "....Doing base_000000000010_(41)_Page_08a\n",
      ".... 10\n",
      "....Doing base_000000000010_(41)_Page_08b\n",
      ".... 20\n",
      "....Doing base_000000000010_(41)_Page_09a\n",
      ".... 21\n",
      "....Doing base_000000000010_(41)_Page_09b\n",
      ".... 20\n",
      "....Doing base_000000000010_(41)_Page_10a\n",
      ".... 22\n",
      "....Doing base_000000000010_(42)_Page_1\n",
      ".... 23\n",
      "....Doing base_000000000010_(44)_Page_1\n",
      ".... 13\n",
      "....Doing base_000000000010_(5)_Page_1\n",
      ".... 40\n",
      "....Doing base_000000000010_(5)_Page_1a\n",
      ".... 38\n",
      "....Doing base_000000000010_(5)_Page_2\n",
      ".... 29\n",
      "....Doing base_000000000010_(50)_Page_1\n",
      ".... 20\n",
      "....Doing base_000000000010_(54)_Page_1\n",
      ".... 21\n",
      "....Doing base_000000000010_(58)_Page_1\n",
      ".... 28\n",
      "....Doing base_000000000010_(58)_Page_2\n",
      ".... 28\n",
      "....Doing base_000000000010_(62)\n",
      ".... 20\n",
      "....Doing base_000000000010_(68)_Page_1\n",
      ".... 25\n",
      "....Doing base_000000000010_(75)_Page_1\n",
      ".... 38\n",
      "....Doing base_000000000010_(75)_Page_2\n",
      ".... 41\n",
      "....Doing base_000000000010_(75)_Page_3\n",
      ".... 41\n",
      "....Doing base_000000000010_(75)_Page_4\n",
      ".... 39\n",
      "....Doing base_000000000010_(78)\n",
      ".... 20\n",
      "....Doing base_000000000010_(9)\n",
      ".... 19\n",
      "....Doing base_000000000010_(90)\n",
      ".... 22\n",
      "....Doing base_000000000010_(90)a\n",
      ".... 20\n",
      "....Doing base_000000000010_(97)\n",
      ".... 33\n",
      "....Doing base_0000000009_1864_Page_07\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_07a\n",
      ".... 21\n",
      "....Doing base_0000000009_1864_Page_12\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_12a\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_15a\n",
      ".... 20\n",
      "....Doing base_0000000009_1864_Page_26\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_31\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_34\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_34a\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_35a\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_37\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_38\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_38a\n",
      ".... 21\n",
      "....Doing base_0000000009_1864_Page_41\n",
      ".... 21\n",
      "....Doing base_0000000009_1864_Page_45a\n",
      ".... 23\n",
      "....Doing base_0000000009_1864_Page_46\n",
      ".... 23\n",
      "....Doing base_0000000009_1864_Page_55a\n",
      ".... 21\n",
      "....Doing base_0000000009_1864_Page_56\n",
      ".... 23\n",
      "....Doing base_0000000009_1864_Page_56a\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_57\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_57a\n",
      ".... 20\n",
      "....Doing base_0000000009_1864_Page_58\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_58a\n",
      ".... 20\n",
      "....Doing base_0000000009_1864_Page_59\n",
      ".... 21\n",
      "....Doing base_0000000009_1864_Page_59a\n",
      ".... 22\n",
      "....Doing base_0000000009_1864_Page_62a\n",
      ".... 22\n",
      "....Doing base_000002_(325)_Page_1\n",
      ".... 25\n",
      "....Doing base_000002_(325)_Page_2\n",
      ".... 20\n",
      "....Doing base_000003_(95)_Page_1\n",
      ".... 27\n",
      "....Doing base_000003_(95)_Page_2\n",
      ".... 20\n",
      "....Doing base_26025_01r\n",
      ".... 23\n",
      "....Doing base_26025_08\n",
      ".... 18\n",
      "....Doing base_26025_09\n",
      ".... 22\n",
      "....Doing base_26066_05\n",
      ".... 18\n",
      "....Doing base_26066_06r\n",
      ".... 18\n",
      "....Doing base_26066_06v\n",
      ".... 13\n",
      "done 100\n",
      "....Doing base_27404_2_138\n",
      ".... 18\n",
      "....Doing base_27404_2_139\n",
      ".... 18\n",
      "....Doing base_27404_2_140\n",
      ".... 14\n",
      "....Doing base_27404_2_141\n",
      ".... 18\n",
      "....Doing base_27404_2_142\n",
      ".... 19\n",
      "....Doing base_27404_2_143\n",
      ".... 19\n",
      "....Doing base_27404_2_144\n",
      ".... 19\n",
      "....Doing base_27404_2_155\n",
      ".... 19\n",
      "....Doing base_27404_2_163\n",
      ".... 21\n",
      "....Doing base_27404_2_164\n",
      ".... 20\n",
      "....Doing base_27404_2_165\n",
      ".... 19\n",
      "....Doing base_27404_2_166\n",
      ".... 21\n",
      "....Doing base_27404_2_167\n",
      ".... 21\n",
      "....Doing base_27404_2_168\n",
      ".... 21\n",
      "....Doing base_27404_2_170\n",
      ".... 21\n",
      "....Doing base_JEH2A1_01r\n",
      ".... 20\n",
      "....Doing base_JEH2A2_17\n",
      ".... 22\n",
      "....Doing base_JEH2A2_18\n",
      ".... 30\n",
      "....Doing base_JEH2A2_19\n",
      ".... 25\n",
      "....Doing base_JEH2A2_20_01\n",
      ".... 28\n",
      "....Doing base_JEH2A2_20_02\n",
      ".... 19\n",
      "....Doing base_JEH2E1_19r\n",
      ".... 18\n",
      "....Doing base_JEH2E1_20_04r\n",
      ".... 18\n",
      "....Doing base_JEH2E1_21_01r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_21_02r\n",
      ".... 15\n",
      "....Doing base_JEH2E1_22r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_23r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_24r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_26r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_27r\n",
      ".... 17\n",
      "....Doing base_JEH2E1_30_08r\n",
      ".... 20\n",
      "....Doing base_JEH2E1_30_10r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_30_13r\n",
      ".... 20\n",
      "....Doing base_JEH2E1_30_15r\n",
      ".... 21\n",
      "....Doing base_JEH2E1_41_01r\n",
      ".... 17\n",
      "....Doing base_JEH2E1_59_06r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_59_07r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_59_28r\n",
      ".... 15\n",
      "....Doing base_JEH2E1_61_08r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_90_03r\n",
      ".... 17\n",
      "....Doing base_JEH2F1_30_01r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_01v\n",
      ".... 19\n",
      "....Doing base_JEH2F1_30_02r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_02v\n",
      ".... 25\n",
      "....Doing base_JEH2F1_30_03r\n",
      ".... 24\n",
      "....Doing base_JEH2F1_30_03v\n",
      ".... 23\n",
      "....Doing base_JEH2F1_30_04r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_04v\n",
      ".... 25\n",
      "....Doing base_JEH2F1_30_05\n",
      ".... 18\n",
      "....Doing base_ME_21_01\n",
      ".... 27\n",
      "....Doing base_ME_21_02\n",
      ".... 26\n",
      "....Doing base_ME_21_03\n",
      ".... 29\n",
      "....Doing base_ME_21_04\n",
      ".... 28\n",
      "....Doing base_ME_21_05\n",
      ".... 29\n",
      "....Doing base_ME_21_06\n",
      ".... 26\n",
      "....Doing base_ME_21_07\n",
      ".... 29\n",
      "....Doing base_ME_21_08\n",
      ".... 29\n",
      "....Doing base_ME_21_09\n",
      ".... 28\n",
      "....Doing base_ME_21_10\n",
      ".... 27\n",
      "....Doing base_ME_21_11\n",
      ".... 30\n",
      "....Doing base_ME_21_12\n",
      ".... 19\n",
      "....Doing base_MUKF1B_01r\n",
      ".... 24\n",
      "....Doing base_MUKF1B_01v\n",
      ".... 23\n",
      "....Doing base_MUKF4A_01_002r\n",
      ".... 41\n",
      "....Doing base_MUKF4A_01_002v\n",
      ".... 26\n",
      "....Doing base_MUKF4A_01_003r\n",
      ".... 32\n",
      "....Doing base_MUKF4A_01_003v\n",
      ".... 18\n",
      "....Doing base_MUKF4A_01_004r\n",
      ".... 30\n",
      "....Doing base_MUKF4A_01_013r\n",
      ".... 26\n",
      "....Doing base_MUKF4A_01_016r\n",
      ".... 25\n",
      "....Doing base_MUKF4A_01_025r\n",
      ".... 26\n",
      "....Doing base_MUKF4A_02_002r\n",
      ".... 32\n",
      "....Doing base_MUKF4A_02_002v\n",
      ".... 33\n",
      "....Doing base_MUKF4A_02_003r\n",
      ".... 35\n",
      "....Doing base_MUKF4A_02_003v\n",
      ".... 33\n",
      "....Doing base_MUKF4A_02_004r\n",
      ".... 34\n",
      "....Doing base_MUKF4A_02_004v\n",
      ".... 33\n",
      "....Doing base_MUKF4A_02_005r\n",
      ".... 38\n",
      "....Doing base_MUKF4A_02_005v\n",
      ".... 30\n",
      "....Doing base_MUKF4A_02_009r\n",
      ".... 32\n",
      "....Doing base_MUKF4A_02_009v\n",
      ".... 33\n",
      "....Doing base_MUKF4A_02_082r\n",
      ".... 36\n",
      "....Doing base_MUKF4A_02_082v\n",
      ".... 36\n",
      "....Doing base_MUKF4A_02_083r\n",
      ".... 36\n",
      "....Doing base_MUKF4A_02_083v\n",
      ".... 36\n",
      "....Doing base_MUKF4A_02_084r\n",
      ".... 33\n",
      "....Doing base_MUKF4A_02_084v\n",
      ".... 35\n",
      "....Doing base_MUKF4A_02_085r\n",
      ".... 36\n",
      "....Doing base_MUKF4A_02_085v\n",
      ".... 35\n",
      "....Doing base_MUKF4A_02_086r\n",
      ".... 37\n",
      "....Doing base_MUKF4A_02_086v\n",
      ".... 14\n",
      "....Doing base_MUKF4A_15_29r\n",
      ".... 31\n",
      "....Doing base_MUKF4A_15_29v\n",
      ".... 31\n",
      "....Doing base_MUKF4A_15_31v\n",
      ".... 30\n",
      "....Doing base_MUKF4A_15_35r\n",
      ".... 33\n",
      "....Doing base_MUKF4A_15_35v\n",
      ".... 40\n",
      "....Doing base_MUKF4A_15_36v\n",
      ".... 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Doing base_MUKF4A_15_37r\n",
      ".... 30\n",
      "....Doing base_MUKF4A_15_37v\n",
      ".... 31\n",
      "....Doing base_MUKF7I_01_019r\n",
      ".... 32\n",
      "done 200\n",
      "....Doing base_MUKF7I_01_021r\n",
      ".... 32\n",
      "....Doing base_MUKF7I_01_021v\n",
      ".... 37\n",
      "....Doing base_MUKF7I_01_022r\n",
      ".... 35\n",
      "....Doing base_MUKF7I_01_022v\n",
      ".... 34\n",
      "....Doing base_MUKF7I_01_023r\n",
      ".... 38\n",
      "....Doing base_MUKF7I_01_023v\n",
      ".... 32\n",
      "....Doing base_MUKF7I_01_024r\n",
      ".... 40\n",
      "....Doing base_MUKF7I_01_024v\n",
      ".... 33\n",
      "....Doing base_MUKF7I_01_025r\n",
      ".... 33\n",
      "....Doing base_MUKF7I_01_025v\n",
      ".... 33\n",
      "....Doing base_MUKF7I_01_026r\n",
      ".... 37\n",
      "....Doing base_MUKF7I_01_026v\n",
      ".... 32\n",
      "....Doing base_MUKF7I_01_027r\n",
      ".... 32\n",
      "....Doing base_MUKF7I_01_027v\n",
      ".... 36\n",
      "Total files in set all_08: 179\n",
      "....Doing base_EAK1A_003r\n",
      ".... 21\n",
      "....Doing base_EAK1A_033r\n",
      ".... 24\n",
      "....Doing base_EAK1A_035r\n",
      ".... 15\n",
      "....Doing base_EAK1A_038r\n",
      ".... 22\n",
      "....Doing base_EAK1A_038v\n",
      ".... 12\n",
      "....Doing base_EAK1A_039r\n",
      ".... 16\n",
      "....Doing base_EAK1A_040r\n",
      ".... 21\n",
      "....Doing base_EAK1A_046_01r\n",
      ".... 19\n",
      "....Doing base_EAK1A_049r\n",
      ".... 18\n",
      "....Doing base_EAK1A_049v\n",
      ".... 21\n",
      "....Doing base_EAK1A_050_01r\n",
      ".... 20\n",
      "....Doing base_EAK1A_050_01v\n",
      ".... 19\n",
      "....Doing base_EAK1A_051r\n",
      ".... 18\n",
      "....Doing base_EAK1A_051v\n",
      ".... 26\n",
      "....Doing base_EAK1A_052r\n",
      ".... 21\n",
      "....Doing base_EAK1A_066r\n",
      ".... 22\n",
      "....Doing base_EAK1A_067r\n",
      ".... 23\n",
      "....Doing base_EAK1A_080r\n",
      ".... 23\n",
      "....Doing base_EAK1A_081r\n",
      ".... 21\n",
      "....Doing base_EAK1A_087r\n",
      ".... 15\n",
      "....Doing base_EAK1A_088r\n",
      ".... 23\n",
      "....Doing base_EAK1A_089r\n",
      ".... 22\n",
      "....Doing base_EAK1A_090_01r\n",
      ".... 19\n",
      "....Doing base_EAK1A_090_02r\n",
      ".... 17\n",
      "....Doing base_EAK1A_095r\n",
      ".... 19\n",
      "....Doing base_EAK1A_099r\n",
      ".... 21\n",
      "....Doing base_EAK1A_137r\n",
      ".... 20\n",
      "....Doing base_EAK1A_137v\n",
      ".... 15\n",
      "....Doing base_EAK1A_138_01r\n",
      ".... 27\n",
      "....Doing base_EAK1A_138_01v\n",
      ".... 24\n",
      "....Doing base_EAK1A_138_02r\n",
      ".... 22\n",
      "....Doing base_EAK1A_140_01r\n",
      ".... 19\n",
      "....Doing base_EAK1A_140_02r\n",
      ".... 23\n",
      "....Doing base_EAK1A_140_03r\n",
      ".... 24\n",
      "....Doing base_EAK1A_140_04r\n",
      ".... 23\n",
      "....Doing base_EAK1A_140_05r\n",
      ".... 24\n",
      "....Doing base_EAK1A_140_06r\n",
      ".... 19\n",
      "....Doing base_EAK1A_140_07r\n",
      ".... 19\n",
      "....Doing base_EAK1A_140_08r\n",
      ".... 22\n",
      "....Doing base_EAK1A_142r\n",
      ".... 21\n",
      "....Doing base_EAK1A_142v\n",
      ".... 20\n",
      "....Doing base_EAK1A_144_02r\n",
      ".... 24\n",
      "....Doing base_EAK1A_144_02v\n",
      ".... 21\n",
      "....Doing base_EAK1A_145r\n",
      ".... 22\n",
      "....Doing base_EAK1A_146r\n",
      ".... 20\n",
      "....Doing base_EAK1A_146v\n",
      ".... 21\n",
      "....Doing base_EAK1A_147r\n",
      ".... 17\n",
      "....Doing base_EAK1A_147v\n",
      ".... 19\n",
      "....Doing base_EAK1A_149r\n",
      ".... 24\n",
      "....Doing base_EAK1A_151r\n",
      ".... 26\n",
      "....Doing base_EAK1A_154_01r\n",
      ".... 12\n",
      "....Doing base_EAK1A_154_01v\n",
      ".... 19\n",
      "....Doing base_EAK1A_154_02r\n",
      ".... 20\n",
      "....Doing base_EAK1A_154_02v\n",
      ".... 14\n",
      "....Doing base_EAK1A_157r\n",
      ".... 26\n",
      "....Doing base_EAK1A_158r\n",
      ".... 24\n",
      "....Doing base_EAK1A_158v\n",
      ".... 30\n",
      "....Doing base_EAK1A_160_01r\n",
      ".... 23\n",
      "....Doing base_EAK1A_161r\n",
      ".... 19\n",
      "....Doing base_EAK1A_162r\n",
      ".... 21\n",
      "....Doing base_EAK1A_165r\n",
      ".... 26\n",
      "....Doing base_EAK1A_169r\n",
      ".... 18\n",
      "....Doing base_EAK1A_169v\n",
      ".... 18\n",
      "....Doing base_EAK1A_170r\n",
      ".... 20\n",
      "....Doing base_EAK1A_170v\n",
      ".... 17\n",
      "....Doing base_EAK1A_171_01r\n",
      ".... 22\n",
      "....Doing base_EAK1A_171_02v\n",
      ".... 21\n",
      "....Doing base_EAK1A_172_01r\n",
      ".... 22\n",
      "....Doing base_EAK1A_178_01r\n",
      ".... 20\n",
      "....Doing base_EAK1A_179r\n",
      ".... 23\n",
      "....Doing base_EAK1A_180r\n",
      ".... 19\n",
      "....Doing base_EAK1A_180v\n",
      ".... 23\n",
      "....Doing base_EAK1A_188r\n",
      ".... 22\n",
      "....Doing base_EAK1A_192r\n",
      ".... 18\n",
      "....Doing base_EAK1A_192v\n",
      ".... 14\n",
      "....Doing base_EAK1A_193_01r\n",
      ".... 21\n",
      "....Doing base_EAK1A_193_02v\n",
      ".... 22\n",
      "....Doing base_EAK1A_195_01r\n",
      ".... 25\n",
      "....Doing base_EAK1A_195_01v\n",
      ".... 24\n",
      "....Doing base_EAK1A_195_02r\n",
      ".... 25\n",
      "....Doing base_EAK1A_195_02v\n",
      ".... 20\n",
      "....Doing base_EAK1A_287_03v\n",
      ".... 19\n",
      "....Doing base_EAK1A_288r\n",
      ".... 25\n",
      "....Doing base_EAK1A_289r\n",
      ".... 31\n",
      "....Doing base_EAK1A_292r\n",
      ".... 25\n",
      "....Doing base_EAK1A_322r\n",
      ".... 17\n",
      "....Doing base_EAK1A_325r\n",
      ".... 20\n",
      "....Doing base_EAK1A_326r\n",
      ".... 22\n",
      "....Doing base_EAK1A_327r\n",
      ".... 18\n",
      "....Doing base_EAK1A_327v\n",
      ".... 15\n",
      "....Doing base_EAK1A_328r\n",
      ".... 16\n",
      "....Doing base_EAK1A_333r\n",
      ".... 20\n",
      "....Doing base_EAK1A_364r\n",
      ".... 32\n",
      "....Doing base_EAK1A_681_01r\n",
      ".... 20\n",
      "....Doing base_EAK1A_681_01v\n",
      ".... 19\n",
      "....Doing base_OLM2_BnabilN3_16_04r\n",
      ".... 38\n",
      "....Doing base_OLM2_BnabilN3_16_04v\n",
      ".... 52\n",
      "done 100\n",
      "....Doing base_OLM2_BnabilN3_16_05r\n",
      ".... 36\n",
      "....Doing base_OLM2_BnabilN3_16_05v\n",
      ".... 43\n",
      "....Doing base_OLM2_BnabilN3_16_06r\n",
      ".... 59\n",
      "....Doing base_OLM2_BnabilN3_16_06v\n",
      ".... 40\n",
      "....Doing base_OLM2_BnabilN3_16_07r\n",
      ".... 58\n",
      "....Doing base_OLM2_BnabilN3_16_07v\n",
      ".... 39\n",
      "....Doing base_OLM2_BnabilN3_16_08r\n",
      ".... 29\n",
      "....Doing base_OLM2_BnabilN3_16_08v\n",
      ".... 41\n",
      "....Doing base_OLM2_BnabilN3_16_09r\n",
      ".... 60\n",
      "....Doing base_OLM2_BnabilN3_16_09v\n",
      ".... 55\n",
      "....Doing base_OLM2_BnabilN3_16_10r\n",
      ".... 42\n",
      "....Doing base_OLM2_BnabilN3_16_10v\n",
      ".... 51\n",
      "....Doing base_OLM2_BnabilN3_16_11r\n",
      ".... 67\n",
      "....Doing base_OLM2_BnabilN3_16_11v\n",
      ".... 60\n",
      "....Doing base_OLM2_BnabilN3_16_12r\n",
      ".... 54\n",
      "....Doing base_OLM2_BnabilN3_16_12v\n",
      ".... 53\n",
      "....Doing base_OLM2_BnabilN3_16_13r\n",
      ".... 62\n",
      "....Doing base_OLM2_BnabilN3_16_13v\n",
      ".... 66\n",
      "....Doing base_OLM2_BnabilN3_16_37r\n",
      ".... 44\n",
      "....Doing base_OLM2_BnabilN3_16_37v\n",
      ".... 37\n",
      "....Doing base_OLM2_BnabilN3_16_38r\n",
      ".... 43\n",
      "....Doing base_OLM2_BnabilN3_16_38v\n",
      ".... 35\n",
      "....Doing base_OLM2_BnabilN3_16_39r\n",
      ".... 35\n",
      "....Doing base_OLM2_BnabilN3_16_39v\n",
      ".... 39\n",
      "....Doing base_OLM2_BnabilN3_16_40v\n",
      ".... 31\n",
      "....Doing base_OLM2_BnabilN3_16_41r\n",
      ".... 34\n",
      "....Doing base_OLM2_BnabilN3_16_41v\n",
      ".... 29\n",
      "....Doing base_OLM2_BnabilN3_16_42r\n",
      ".... 40\n",
      "....Doing base_OLM2_BnabilN3_16_42v\n",
      ".... 31\n",
      "....Doing base_OLM2_BnabilN3_16_43r\n",
      ".... 37\n",
      "....Doing base_OLM2_BnabilN3_16_43v\n",
      ".... 40\n",
      "....Doing base_OLM2_BnabilN3_16_44r\n",
      ".... 28\n",
      "....Doing base_OLM2_BnabilN3_16_44v\n",
      ".... 23\n",
      "....Doing base_OLM2_BnabilN3_16_45r\n",
      ".... 27\n",
      "....Doing base_OLM2_BnabilN3_16_45v\n",
      ".... 35\n",
      "....Doing base_OLM2_BnabilN3_16_46r\n",
      ".... 35\n",
      "....Doing base_OLM2_BnabilN3_16_46v\n",
      ".... 26\n",
      "....Doing base_OLM2_BnabilN3_16_47r\n",
      ".... 32\n",
      "....Doing base_OLM2_BnabilN3_16_47v\n",
      ".... 32\n",
      "....Doing base_OLM2_BnabilN3_16_48r\n",
      ".... 31\n",
      "....Doing base_OLM2_BnabilN3_16_48v\n",
      ".... 28\n",
      "....Doing base_OLM2_BnabilN3_16_49r\n",
      ".... 28\n",
      "....Doing base_OLM2_BnabilN3_16_49v\n",
      ".... 24\n",
      "....Doing base_OLM2_Kartaba_N8_01_008r\n",
      ".... 27\n",
      "....Doing base_OLM2_Kartaba_N8_01_008v\n",
      ".... 45\n",
      "....Doing base_OLM2_Kartaba_N8_01_009r\n",
      ".... 45\n",
      "....Doing base_OLM2_Kartaba_N8_01_009v\n",
      ".... 73\n",
      "....Doing base_OLM2_Kartaba_N8_01_010r\n",
      ".... 77\n",
      "....Doing base_OLM2_Kartaba_N8_01_010v\n",
      ".... 56\n",
      "....Doing base_OLM2_Kartaba_N8_01_011r\n",
      ".... 66\n",
      "....Doing base_OLM2_Kartaba_N8_01_011v\n",
      ".... 38\n",
      "....Doing base_OLM2_Kartaba_N8_01_012r\n",
      ".... 67\n",
      "....Doing base_OLM2_Kartaba_N8_01_012v\n",
      ".... 52\n",
      "....Doing base_OLM2_Kartaba_N8_01_013r\n",
      ".... 52\n",
      "....Doing base_OLM2_Kartaba_N8_01_013v\n",
      ".... 82\n",
      "....Doing base_OLM2_Kartaba_N8_01_014r\n",
      ".... 51\n",
      "....Doing base_OLM2_Kartaba_N8_01_014v\n",
      ".... 58\n",
      "....Doing base_OLM2_Kartaba_N8_01_015r\n",
      ".... 47\n",
      "....Doing base_OLM2_Kartaba_N8_01_015v\n",
      ".... 64\n",
      "....Doing base_OLM2_Kartaba_N8_01_016r\n",
      ".... 47\n",
      "....Doing base_OLM2_Kartaba_N8_01_016v\n",
      ".... 64\n",
      "....Doing base_OLM2_Kartaba_N8_01_017r\n",
      ".... 60\n",
      "....Doing base_OLM2_Kartaba_N8_01_017v\n",
      ".... 48\n",
      "....Doing base_OLM2_Machmouche_N8_03_004r\n",
      ".... 61\n",
      "....Doing base_OLM2_Machmouche_N8_03_004v\n",
      ".... 49\n",
      "....Doing base_OLM2_Machmouche_N8_03_005r\n",
      ".... 61\n",
      "....Doing base_OLM2_Machmouche_N8_03_006r\n",
      ".... 77\n",
      "....Doing base_OLM2_Machmouche_N8_03_007r\n",
      ".... 53\n",
      "....Doing base_OLM2_Machmouche_N8_03_007v\n",
      ".... 80\n",
      "....Doing base_OLM2_Machmouche_N8_03_008r\n",
      ".... 59\n",
      "....Doing base_OLM2_Machmouche_N8_03_008v\n",
      ".... 49\n",
      "....Doing base_OLM2_Machmouche_N8_03_009r\n",
      ".... 69\n",
      "....Doing base_OLM2_Machmouche_N8_03_009v\n",
      ".... 62\n",
      "....Doing base_OLM2_Machmouche_N8_03_010r\n",
      ".... 72\n",
      "....Doing base_OLM2_Machmouche_N8_03_010v\n",
      ".... 51\n",
      "....Doing base_OLM2_Machmouche_N8_03_011r\n",
      ".... 66\n",
      "....Doing base_OLM2_Machmouche_N8_03_012r\n",
      ".... 63\n",
      "....Doing base_OLM2_Machmouche_N8_03_012v\n",
      ".... 67\n",
      "....Doing base_OLM2_Machmouche_N8_03_013r\n",
      ".... 66\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# This routine is for scribe arabic\n",
    "def preprocess_scribe():\n",
    "    # Make sure there is a / at end of path\n",
    "    paths = ['/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr_ver2/']\n",
    "    #set_name = ['all_04',  'all_05',   'all_06']\n",
    "    #batches = ['batch_04', 'batch_05', 'batch_06']\n",
    "    set_name = ['all_07', 'all_08']\n",
    "    batches = ['batch_07', 'batch_08']\n",
    "    input_file_dir = '/home/msaeed3/mehreen/datasets/scribeArabic/' \n",
    "    \n",
    "    for set_to_use, batch in zip(set_name, batches):\n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file=input_file_dir+f'{batch}.csv')\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "            \n",
    "preprocess_scribe()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4dba9",
   "metadata": {},
   "source": [
    "# Uncomment this cell to check for a single file\n",
    "\n",
    "out_path = '/home/msaeed3/mehreen/datasets/MoiseK/'\n",
    "\n",
    "check_file = '15805_01r_cropped.jpg' \n",
    "set_name = 'all_05'\n",
    "csv_file = '/home/msaeed3/mehreen/datasets/scribeArabic/batch_05.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "\n",
    "df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "\n",
    "files = df.image_file\n",
    "\n",
    "all_ground_truth = []\n",
    "for img_path in set(files):\n",
    "    \n",
    "    if check_file in img_path:\n",
    "        print('***')\n",
    "        para_df = df[df.image_file == img_path]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "        valid_region = any(s in para_df['region_type'][0] for s in ['paragraph','text'])\n",
    "        if not valid_region:\n",
    "            print('NOT DONE')\n",
    "        json_path = handle_single_para(para_df, out_path+set_name, flip=True)       \n",
    "        print(json_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16415cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpara_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'para_df' is not defined"
     ]
    }
   ],
   "source": [
    "para_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19efa",
   "metadata": {},
   "source": [
    "# This is for priinting the json file in readable format\n",
    "\n",
    "json_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train.json'\n",
    "with open(json_file, 'r') as fin:\n",
    "    json_obj = json.load(fin)\n",
    "\n",
    "out_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train_readable.json'\n",
    "with open(out_file, 'w') as fout:\n",
    "    json.dump(json_obj, fout, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
