{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import parse_PAGE\n",
    "import cv2\n",
    "import line_extraction\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import codecs\n",
    "from svgpathtools import Path, Line\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6462894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_mapping(img, ts, path, offset_1, offset_2, max_min = None, cube_size = None):\n",
    "    # cube_size = 80\n",
    "\n",
    "    offset_1_pts = []\n",
    "    offset_2_pts = []\n",
    "    # for t in ts:\n",
    "    for i in range(len(ts)):\n",
    "        t = ts[i]\n",
    "        pt = path.point(t)\n",
    "\n",
    "        norm = None\n",
    "        if i == 0:\n",
    "            norm = normal(pt, path.point(ts[i+1]))\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        elif i == len(ts)-1:\n",
    "            norm = normal(path.point(ts[i-1]), pt)\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        else:\n",
    "            norm1 = normal(path.point(ts[i-1]), pt)\n",
    "            norm1 = norm1 / dis(complex(0,0), norm1)\n",
    "            norm2 = normal(pt, path.point(ts[i+1]))\n",
    "            norm2 = norm2 / dis(complex(0,0), norm2)\n",
    "\n",
    "            norm = (norm1 + norm2)/2\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "\n",
    "        offset_vector1 = offset_1 * norm\n",
    "        offset_vector2 = offset_2 * norm\n",
    "\n",
    "        pt1 = pt + offset_vector1\n",
    "        pt2 = pt + offset_vector2\n",
    "\n",
    "        offset_1_pts.append(complexToNpPt(pt1))\n",
    "        offset_2_pts.append(complexToNpPt(pt2))\n",
    "\n",
    "    offset_1_pts = np.array(offset_1_pts)\n",
    "    offset_2_pts = np.array(offset_2_pts)\n",
    "\n",
    "    h,w = img.shape[:2]\n",
    "\n",
    "    offset_source2 = np.array([(cube_size*i, 0) for i in range(len(offset_1_pts))], dtype=np.float32)\n",
    "    offset_source1 = np.array([(cube_size*i, cube_size) for i in range(len(offset_2_pts))], dtype=np.float32)\n",
    "\n",
    "    offset_source1 = offset_source1[::-1]\n",
    "    offset_source2 = offset_source2[::-1]\n",
    "\n",
    "    source = np.concatenate([offset_source1, offset_source2])\n",
    "    destination = np.concatenate([offset_1_pts, offset_2_pts])\n",
    "\n",
    "    source = source[:,::-1]\n",
    "    destination = destination[:,::-1]\n",
    "\n",
    "    n_w = int(offset_source2[:,0].max())\n",
    "    n_h = int(cube_size)\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:n_h, 0:n_w]\n",
    "\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    rectified_to_warped_x = map_x_32\n",
    "    rectified_to_warped_y = map_y_32\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:h, 0:w]\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(h,w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(h,w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    warped_to_rectified_x = map_x_32\n",
    "    warped_to_rectified_y = map_y_32\n",
    "\n",
    "    return rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min\n",
    "\n",
    "\n",
    "def dis(pt1, pt2):\n",
    "    a = (pt1.real - pt2.real)**2\n",
    "    b = (pt1.imag - pt2.imag)**2\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "def complexToNpPt(pt):\n",
    "    return np.array([pt.real, pt.imag], dtype=np.float32)\n",
    "\n",
    "def normal(pt1, pt2):\n",
    "    dif = pt1 - pt2\n",
    "    return complex(-dif.imag, dif.real)\n",
    "\n",
    "def find_t_spacing(path, cube_size):\n",
    "    print('path:', path)\n",
    "    print('cubesize', cube_size)\n",
    "    l = path.length()\n",
    "    error = 0.01\n",
    "    init_step_size = cube_size / l\n",
    "\n",
    "    last_t = 0\n",
    "    cur_t = 0\n",
    "    pts = []\n",
    "    ts = [0]\n",
    "    pts.append(complexToNpPt(path.point(cur_t)))\n",
    "    path_lookup = {}\n",
    "    for target in np.arange(cube_size, int(l), cube_size):\n",
    "        step_size = init_step_size\n",
    "        for i in range(1000):\n",
    "            cur_length = dis(path.point(last_t), path.point(cur_t))\n",
    "            if np.abs(cur_length - cube_size) < error:\n",
    "                break\n",
    "\n",
    "            step_t = min(cur_t + step_size, 1.0)\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_t = max(cur_t - step_size, 0.0)\n",
    "            step_t = max(step_t, last_t)\n",
    "            step_t = max(step_t, 1.0)\n",
    "\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_size = step_size / 2.0\n",
    "\n",
    "        last_t = cur_t\n",
    "\n",
    "        ts.append(cur_t)\n",
    "        pts.append(complexToNpPt(path.point(cur_t)))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "def find_best_xml(list_of_files, filename):\n",
    "\n",
    "    if len(list_of_files) <= 1:\n",
    "        return list_of_files\n",
    "\n",
    "    print(\"Selecting multiple options from:\")\n",
    "\n",
    "    line_cnts = []\n",
    "    for xml_path in list_of_files:\n",
    "        test_xml_path = os.path.join(xml_path, filename+\".xml\")\n",
    "        print(test_xml_path)\n",
    "        with open(test_xml_path) as f:\n",
    "            num_lines = sum(1 for line in f.readlines() if len(line.strip())>0)\n",
    "        line_cnts.append((num_lines, xml_path))\n",
    "    line_cnts.sort(key=lambda x:x[0], reverse=True)\n",
    "    print(\"Sorted by line count...\")\n",
    "    ret = [l[1] for l in line_cnts]\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_int_tuples(df_col):\n",
    "    tmp_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        item = eval(item)    \n",
    "        tmp_col.append([(round(x[0]), round(x[1])) for x in item])\n",
    "    return tmp_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_single_para(para_df, output_directory):\n",
    "\n",
    "    output_data = []\n",
    "    num_lines = len(para_df)\n",
    "    img = cv2.imread(INPUT_PATH + para_df.image_file.iloc[0])\n",
    "    all_lines = \"\"\n",
    "    basename = 'base_' + para_df.image_file.iloc[0]\n",
    "    print(basename)\n",
    "    # get rid of png extension\n",
    "    basename = basename[:-4]\n",
    "    for region in [0]:\n",
    "        region_output_data = []\n",
    "        print('in region', region)\n",
    "        for ind, line in enumerate(para_df.line_number):\n",
    "            print('in inner ind line', ind, line)\n",
    "            line_mask = line_extraction.extract_region_mask(img, para_df.polygon_pts[ind])\n",
    "            masked_img = img.copy()\n",
    "            masked_img[line_mask==0] = 0\n",
    "\n",
    "            summed_axis0 = (masked_img.astype(float) / 255).sum(axis=0)\n",
    "            summed_axis1 = (masked_img.astype(float) / 255).sum(axis=1)\n",
    "\n",
    "            non_zero_cnt0 = np.count_nonzero(summed_axis0) / float(len(summed_axis0))\n",
    "            non_zero_cnt1 = np.count_nonzero(summed_axis1) / float(len(summed_axis1))\n",
    "\n",
    "            avg_height0 = np.median(summed_axis0[summed_axis0 != 0])\n",
    "            avg_height1 = np.median(summed_axis1[summed_axis1 != 0])\n",
    "\n",
    "            avg_height = min(avg_height0, avg_height1)\n",
    "            if non_zero_cnt0 > non_zero_cnt1:\n",
    "                target_step_size = avg_height0\n",
    "            else:\n",
    "                target_step_size = avg_height1\n",
    "\n",
    "            paths = []\n",
    "            for i in range(len(para_df.baseline[ind])-1):\n",
    "                i_1 = i+1\n",
    "\n",
    "                p1 = para_df.baseline[ind][i]\n",
    "                p2 = para_df.baseline[ind][i_1]\n",
    "\n",
    "                p1_c = complex(*p1)\n",
    "                p2_c = complex(*p2)\n",
    "\n",
    "\n",
    "                paths.append(Line(p1_c, p2_c))\n",
    "\n",
    "\n",
    "            # Add a bit on the end\n",
    "            tan = paths[-1].unit_tangent(1.0)\n",
    "            p3_c = p2_c + target_step_size * tan\n",
    "            paths.append(Line(p2_c, p3_c))\n",
    "\n",
    "            path = Path(*paths)\n",
    "\n",
    "            ts = find_t_spacing(path, target_step_size)\n",
    "#            print('....ts', ts)\n",
    "#            print('....path', path)\n",
    "\n",
    "            #Changing this causes issues in pretraining - not sure why\n",
    "            target_height = 32\n",
    "\n",
    "            rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 0, -2*target_step_size, cube_size = target_height)\n",
    "            warped_above = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "            rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 2*target_step_size, 0, cube_size = target_height)\n",
    "            warped_below = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "            above_scale =  np.max((warped_above.astype(float) / 255).sum(axis=0))\n",
    "            below_scale = np.max((warped_below.astype(float) / 255).sum(axis=0))\n",
    "\n",
    "            ab_sum = above_scale + below_scale\n",
    "            above = target_step_size * (above_scale/ab_sum)\n",
    "            below = target_step_size * (below_scale/ab_sum)\n",
    "\n",
    "            above = target_step_size * (above_scale/(target_height/2.0))\n",
    "            below = target_step_size * (below_scale/(target_height/2.0))\n",
    "            target_step_size = above + below\n",
    "            ts = find_t_spacing(path, target_step_size)\n",
    "\n",
    "            rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, below, -above, cube_size=target_height)\n",
    "\n",
    "            ####MEHREEN COMMENT to prevent image from flipping\n",
    "            #rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "            #rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "            ###END MEHREEN COMMENT\n",
    "            warped_to_rectified_x = warped_to_rectified_x[::-1,::-1]\n",
    "            warped_to_rectified_y = warped_to_rectified_y[::-1,::-1]\n",
    "\n",
    "            warped = cv2.remap(img, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(255,255,255))\n",
    "\n",
    "            ####MEHREEN ADD\n",
    "            # Want to prevent image warping but we want coordinates to be warped\n",
    "            rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "            rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "\n",
    "            #### END MEHREEN ADD\n",
    "            \n",
    "            \n",
    "            mapping = np.stack([rectified_to_warped_y, rectified_to_warped_x], axis=2)\n",
    "\n",
    "            top_left = mapping[0,0,:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "            btm_right = mapping[min(mapping.shape[0]-1, target_height-1), min(mapping.shape[1]-1, target_height-1),:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "\n",
    "\n",
    "            line_points = []\n",
    "            for i in range(0,mapping.shape[1],target_height):\n",
    "\n",
    "                x0 = float(rectified_to_warped_x[0,i])\n",
    "                x1 = float(rectified_to_warped_x[-1,i])\n",
    "\n",
    "                y0 = float(rectified_to_warped_y[0,i])\n",
    "                y1 = float(rectified_to_warped_y[-1,i])\n",
    "\n",
    "                line_points.append({\n",
    "                    \"x0\": x0, #MEhreen change x0,\n",
    "                    \"x1\": x1, #Mehreen change x1,\n",
    "                    \"y0\": y1, #Mehreen change from y0,\n",
    "                    \"y1\": y0, #Mehreen change from y1\n",
    "                })\n",
    "                \n",
    "                \n",
    "                                \n",
    "            ###Mehreen add for viewing\n",
    "\n",
    " #           plt.imshow(img) # or display line warped\n",
    " #           print(\"****\", line_points)\n",
    " #           for coord in line_points:\n",
    " #               x = coord[\"x0\"]\n",
    " #               y = coord[\"y0\"]\n",
    " #               x1 = coord[\"x1\"]\n",
    " #               y1 = coord[\"y1\"]\n",
    "                #rect = patches.Rectangle((x, y), np.abs(x-coord[2]), np.abs(y-coord[3]), facecolor='green')\n",
    " #               rect = patches.Rectangle((x, y), 10, 10, facecolor='blue')\n",
    " #               rect1 = patches.Rectangle((x1, y1), 10, 10, facecolor='red')\n",
    " #               plt.gca().add_patch(rect)  \n",
    " #               plt.gca().add_patch(rect1)\n",
    " #           rect0 = patches.Rectangle((line_points[0][\"x0\"], line_points[0][\"y0\"]), 10, 10, facecolor='yellow') \n",
    " #           plt.gca().add_patch(rect0)\n",
    " #           plt.show()\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "            output_file = os.path.join(output_directory, \n",
    "                          basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            warp_output_file = os.path.join(output_directory, basename, \"{}-{}.png\".format(basename, line))\n",
    "            warp_output_file_save = os.path.join(basename, \"{}-{}.png\".format(basename, str(len(region_output_data))))\n",
    "            save_file = os.path.join(basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            region_output_data.append({\n",
    "                \"gt\": para_df.ground_truth[ind],\n",
    "                \"image_path\": save_file,\n",
    "                \"sol\": line_points[0],\n",
    "                \"lf\": line_points,\n",
    "                \"hw_path\": warp_output_file #MEhreen commentwarp_output_file_save\n",
    "            })\n",
    "            #print('****', output_file)\n",
    "            if not os.path.exists(os.path.dirname(output_file)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(output_file))\n",
    "                except OSError as exc:\n",
    "                    raise Exception(\"Could not write file\")\n",
    "\n",
    "            cv2.imwrite(warp_output_file, warped)\n",
    "\n",
    "        output_data.extend(region_output_data)\n",
    "\n",
    "    output_data_path =os.path.join(output_directory, basename, \"{}.json\".format(basename))\n",
    "    if not os.path.exists(os.path.dirname(output_data_path)):\n",
    "        os.makedirs(os.path.dirname(output_data_path))\n",
    "\n",
    "    with open(output_data_path, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "\n",
    "    return output_data_path    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_polygon(p):\n",
    "    if len(p) == 8 or len(p) == 7:\n",
    "        poly = p[4:]\n",
    "        poly.extend(p[0:4])\n",
    "        return poly\n",
    "    if len(p) == 4:\n",
    "        poly = [p[2], p[3], p[0], p[1]]\n",
    "        return poly\n",
    "    else:\n",
    "        print(\"something wrong\", p)\n",
    "        return []\n",
    "    \n",
    "def rotate_poly_list(df_col):\n",
    "    poly_list = [rotate_polygon(p) for p in df_col]\n",
    "    return poly_list\n",
    "\n",
    "\n",
    "def rotate_baseline_list(df_col):\n",
    "    b_list = [b[::-1] for b in df_col]\n",
    "    return b_list\n",
    "    \n",
    "def process_khaleej_dir(total_to_process=100):\n",
    "    df = pd.read_csv(INPUT_PATH+PARA_CSV)\n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    paragraph_number = df.paragraph_number.to_numpy()\n",
    "    print('Total paragraphs: ', np.max(paragraph_number))\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for i in set(paragraph_number):\n",
    "        \n",
    "        para_df = df[df.paragraph_number == i]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        img_path = INPUT_PATH + para_df.image_file.iloc[0]\n",
    "        \n",
    "        json_path = handle_single_para(para_df, OUTPUT_DIR)       \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth) == total_to_process:\n",
    "            break\n",
    "        print('done', i)\n",
    "    return all_ground_truth\n",
    "\n",
    "process_khaleej_dir(1)\n",
    "sldfjadsflk\n",
    "   \n",
    "    \n",
    "def main_synthetic_khaleej_preprocess():\n",
    "    \n",
    "    training_output_json = OUTPUT_DIR + TRAIN_OUT_JSON\n",
    "    validation_output_json = OUTPUT_DIR + VALID_OUT_JSON\n",
    "    \n",
    "    all_ground_truth = process_khaleej_dir()\n",
    "    all_ground_truth.sort()\n",
    "\n",
    "\n",
    "    training_list = all_ground_truth[:70]\n",
    "    validation_list = all_ground_truth[70:]\n",
    "\n",
    "    print(\"Training Size:\", len(training_list))\n",
    "    print(\"Validation Size:\", len(validation_list))\n",
    "\n",
    "    with open(training_output_json, 'w') as f:\n",
    "        json.dump(training_list, f)\n",
    "\n",
    "    with open(validation_output_json, 'w') as f:\n",
    "        json.dump(validation_list, f)\n",
    "\n",
    "#df = pd.read_csv(INPUT_PATH+PARA_CSV)\n",
    "#df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "#df.baseline = rotate_baseline_list(df.baseline)\n",
    "#print(df.baseline[0])\n",
    "\n",
    "#para_df = df[df.paragraph_number == 2]\n",
    "#para_df = para_df.copy()\n",
    "#para_df = para_df.reset_index(drop=True)\n",
    "#print(para_df)\n",
    "\n",
    "\n",
    "main_synthetic_khaleej_preprocess()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374673a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = '/home/msaeed3/mehreen/datasets/synthetic/khaleej_local_paragraphs_c/'\n",
    "PARA_CSV = 'khaleej_paragraph_prepared_c.csv'\n",
    "OUTPUT_DIR = '/home/msaeed3/mehreen/datasets/synthetic/khaleej_local_paragraphs_c/sfr/'\n",
    "TRAIN_OUT_JSON = 'khaleej_paragraph_train.json'\n",
    "VALID_OUT_JSON = 'khaleej_paragraph_valid.json'\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH+PARA_CSV)\n",
    "\n",
    "paragraph_number = df.paragraph_number.to_numpy()\n",
    "print('max para number', np.max(paragraph_number))\n",
    "#print(set(paragraph_number))\n",
    "print(df.image_file[0])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_df = df[df.paragraph_number == 1]\n",
    "para_df = para_df.copy()\n",
    "para_df = para_df.reset_index(drop=True)\n",
    "img_path = INPUT_PATH + para_df.image_file.iloc[0]\n",
    "\n",
    "json_path = handle_single_para(para_df, OUTPUT_DIR) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
