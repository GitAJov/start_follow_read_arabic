{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import parse_PAGE\n",
    "import cv2\n",
    "import line_extraction\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import codecs\n",
    "from svgpathtools import Path, Line\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "HT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6462894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_mapping(img, ts, path, offset_1, offset_2, max_min = None, cube_size = None):\n",
    "    # cube_size = 80\n",
    "\n",
    "    offset_1_pts = []\n",
    "    offset_2_pts = []\n",
    "    # for t in ts:\n",
    "    for i in range(len(ts)):\n",
    "        t = ts[i]\n",
    "        pt = path.point(t)\n",
    "\n",
    "        norm = None\n",
    "        if i == 0:\n",
    "            norm = normal(pt, path.point(ts[i+1]))\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        elif i == len(ts)-1:\n",
    "            norm = normal(path.point(ts[i-1]), pt)\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        else:\n",
    "            norm1 = normal(path.point(ts[i-1]), pt)\n",
    "            norm1 = norm1 / dis(complex(0,0), norm1)\n",
    "            norm2 = normal(pt, path.point(ts[i+1]))\n",
    "            norm2 = norm2 / dis(complex(0,0), norm2)\n",
    "\n",
    "            norm = (norm1 + norm2)/2\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "\n",
    "        offset_vector1 = offset_1 * norm\n",
    "        offset_vector2 = offset_2 * norm\n",
    "\n",
    "        pt1 = pt + offset_vector1\n",
    "        pt2 = pt + offset_vector2\n",
    "\n",
    "        offset_1_pts.append(complexToNpPt(pt1))\n",
    "        offset_2_pts.append(complexToNpPt(pt2))\n",
    "\n",
    "    offset_1_pts = np.array(offset_1_pts)\n",
    "    offset_2_pts = np.array(offset_2_pts)\n",
    "\n",
    "    h,w = img.shape[:2]\n",
    "\n",
    "    offset_source2 = np.array([(cube_size*i, 0) for i in range(len(offset_1_pts))], dtype=np.float32)\n",
    "    offset_source1 = np.array([(cube_size*i, cube_size) for i in range(len(offset_2_pts))], dtype=np.float32)\n",
    "\n",
    "    offset_source1 = offset_source1[::-1]\n",
    "    offset_source2 = offset_source2[::-1]\n",
    "\n",
    "    source = np.concatenate([offset_source1, offset_source2])\n",
    "    destination = np.concatenate([offset_1_pts, offset_2_pts])\n",
    "\n",
    "    source = source[:,::-1]\n",
    "    destination = destination[:,::-1]\n",
    "\n",
    "    n_w = int(offset_source2[:,0].max())\n",
    "    n_h = int(cube_size)\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:n_h, 0:n_w]\n",
    "\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    rectified_to_warped_x = map_x_32\n",
    "    rectified_to_warped_y = map_y_32\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:h, 0:w]\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(h,w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(h,w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    warped_to_rectified_x = map_x_32\n",
    "    warped_to_rectified_y = map_y_32\n",
    "\n",
    "    return rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min\n",
    "\n",
    "\n",
    "def dis(pt1, pt2):\n",
    "    a = (pt1.real - pt2.real)**2\n",
    "    b = (pt1.imag - pt2.imag)**2\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "def complexToNpPt(pt):\n",
    "    return np.array([pt.real, pt.imag], dtype=np.float32)\n",
    "\n",
    "def normal(pt1, pt2):\n",
    "    dif = pt1 - pt2\n",
    "    return complex(-dif.imag, dif.real)\n",
    "\n",
    "def find_t_spacing(path, cube_size):\n",
    "\n",
    "    l = path.length()\n",
    "    error = 0.01\n",
    "    init_step_size = cube_size / l\n",
    "\n",
    "    last_t = 0\n",
    "    cur_t = 0\n",
    "    pts = []\n",
    "    ts = [0]\n",
    "    pts.append(complexToNpPt(path.point(cur_t)))\n",
    "    path_lookup = {}\n",
    "    for target in np.arange(cube_size, int(l), cube_size):\n",
    "        step_size = init_step_size\n",
    "        for i in range(1000):\n",
    "            cur_length = dis(path.point(last_t), path.point(cur_t))\n",
    "            if np.abs(cur_length - cube_size) < error:\n",
    "                break\n",
    "\n",
    "            step_t = min(cur_t + step_size, 1.0)\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_t = max(cur_t - step_size, 0.0)\n",
    "            step_t = max(step_t, last_t)\n",
    "            step_t = max(step_t, 1.0)\n",
    "\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_size = step_size / 2.0\n",
    "\n",
    "        last_t = cur_t\n",
    "\n",
    "        ts.append(cur_t)\n",
    "        pts.append(complexToNpPt(path.point(cur_t)))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    return ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbd6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basename(img_name):\n",
    "    head, tail = os.path.split(img_name)\n",
    "    basename = 'base_' + tail\n",
    "    basename = basename[:-4]\n",
    "    return basename\n",
    "\n",
    "def handle_single_para(para_df, output_directory, flip=False):\n",
    "    \n",
    "    output_data = []\n",
    "    num_lines = len(para_df)\n",
    "    print('....', num_lines)\n",
    "    if os.path.exists(para_df.image_file.iloc[0]):\n",
    "        img = cv2.imread(para_df.image_file.iloc[0])\n",
    "        if flip:\n",
    "            img = cv2.flip(img, 1)\n",
    "    else:\n",
    "        print('....File not found', para_df.image_file.iloc[0])\n",
    "        return ''\n",
    "    basename = get_basename(para_df.image_file.iloc[0])    \n",
    "    \n",
    "    all_lines = \"\"\n",
    "    \n",
    "    # get rid of png/jpg extension\n",
    "    \n",
    "    for region in [0]:\n",
    "        region_output_data = []\n",
    "        #print('in region', region)\n",
    "        for ind, line in enumerate(para_df.line_number):\n",
    "            if len(para_df.polygon_pts[ind]) == 0:\n",
    "                print('No polygon pts in img', para_df.image_file.iloc[0][-15:],\n",
    "                      'line number', line)\n",
    "                continue\n",
    "            #print('....ind, line', ind, line)\n",
    "            line_mask = line_extraction.extract_region_mask(img, para_df.polygon_pts[ind])\n",
    "            masked_img = img.copy()\n",
    "            masked_img[line_mask==0] = 0\n",
    "\n",
    "            summed_axis0 = (masked_img.astype(float) / 255).sum(axis=0)\n",
    "            summed_axis1 = (masked_img.astype(float) / 255).sum(axis=1)\n",
    "\n",
    "            non_zero_cnt0 = np.count_nonzero(summed_axis0) / float(len(summed_axis0))\n",
    "            non_zero_cnt1 = np.count_nonzero(summed_axis1) / float(len(summed_axis1))\n",
    "\n",
    "            avg_height0 = np.median(summed_axis0[summed_axis0 != 0])\n",
    "            avg_height1 = np.median(summed_axis1[summed_axis1 != 0])\n",
    "\n",
    "            avg_height = min(avg_height0, avg_height1)\n",
    "            if non_zero_cnt0 > non_zero_cnt1:\n",
    "                target_step_size = avg_height0\n",
    "            else:\n",
    "                target_step_size = avg_height1\n",
    "\n",
    "            paths = []\n",
    "            for i in range(len(para_df.baseline[ind])-1):\n",
    "                i_1 = i+1\n",
    "\n",
    "                p1 = para_df.baseline[ind][i]\n",
    "                p2 = para_df.baseline[ind][i_1]\n",
    "\n",
    "                p1_c = complex(*p1)\n",
    "                p2_c = complex(*p2)\n",
    "\n",
    "\n",
    "                paths.append(Line(p1_c, p2_c))\n",
    "\n",
    "\n",
    "            if len(paths) == 0:\n",
    "                print('Path length is 0', 'for ind', ind)\n",
    "                continue\n",
    "            # Add a bit on the end\n",
    "            tan = paths[-1].unit_tangent(1.0)\n",
    "            p3_c = p2_c + target_step_size * tan\n",
    "            paths.append(Line(p2_c, p3_c))\n",
    "\n",
    "            path = Path(*paths)\n",
    "            \n",
    "            try:\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                \n",
    "                #Changing this causes issues in pretraining - not sure why\n",
    "                target_height = HT\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 0, -2*target_step_size, cube_size = target_height)\n",
    "                warped_above = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 2*target_step_size, 0, cube_size = target_height)\n",
    "                warped_below = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                above_scale =  np.max((warped_above.astype(float) / 255).sum(axis=0))\n",
    "                below_scale = np.max((warped_below.astype(float) / 255).sum(axis=0))\n",
    "\n",
    "\n",
    "                \n",
    "                ab_sum = above_scale + below_scale\n",
    "                above = target_step_size * (above_scale/ab_sum)\n",
    "                below = target_step_size * (below_scale/ab_sum)\n",
    "\n",
    "                above = target_step_size * (above_scale/(target_height/2.0))\n",
    "                below = target_step_size * (below_scale/(target_height/2.0))\n",
    "                target_step_size = above + below\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                if len(ts) <= 1:\n",
    "                    print('Not doing line', line)\n",
    "                    continue\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, below, -above, cube_size=target_height)\n",
    "\n",
    "                rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "                rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "\n",
    "                warped_to_rectified_x = warped_to_rectified_x[::-1,::-1]\n",
    "                warped_to_rectified_y = warped_to_rectified_y[::-1,::-1]\n",
    "\n",
    "                warped = cv2.remap(img, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(255,255,255))\n",
    "            except:\n",
    "                print('Not doing line', line)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            mapping = np.stack([rectified_to_warped_y, rectified_to_warped_x], axis=2)\n",
    "\n",
    "            top_left = mapping[0,0,:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "            btm_right = mapping[min(mapping.shape[0]-1, target_height-1), min(mapping.shape[1]-1, target_height-1),:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "\n",
    "\n",
    "            line_points = []\n",
    "            for i in range(0,mapping.shape[1],target_height):\n",
    "\n",
    "                x0 = float(rectified_to_warped_x[0,i])\n",
    "                x1 = float(rectified_to_warped_x[-1,i])\n",
    "\n",
    "                y0 = float(rectified_to_warped_y[0,i])\n",
    "                y1 = float(rectified_to_warped_y[-1,i])\n",
    "\n",
    "                line_points.append({\n",
    "                    \"x0\": x0, \n",
    "                    \"x1\": x1, \n",
    "                    \"y0\": y0, \n",
    "                    \"y1\": y1, \n",
    "                })\n",
    "                \n",
    "                \n",
    "                                \n",
    "            ###Mehreen add for viewing\n",
    "\n",
    "#            plt.imshow(img) # or display line warped\n",
    "#            print(\"****\", line_points)\n",
    "#            for coord in line_points:\n",
    "#                x = coord[\"x0\"]\n",
    "#                y = coord[\"y0\"]\n",
    "#                x1 = coord[\"x1\"]\n",
    "#                y1 = coord[\"y1\"]\n",
    "                #rect = patches.Rectangle((x, y), np.abs(x-coord[2]), np.abs(y-coord[3]), facecolor='green')\n",
    "#                rect = patches.Rectangle((x, y), 10, 10, facecolor='blue')\n",
    "#                rect1 = patches.Rectangle((x1, y1), 10, 10, facecolor='red')\n",
    "#                plt.gca().add_patch(rect)  \n",
    "#                plt.gca().add_patch(rect1)\n",
    "#            rect0 = patches.Rectangle((line_points[0][\"x0\"], line_points[0][\"y0\"]), 10, 10, facecolor='yellow') \n",
    "#            plt.gca().add_patch(rect0)\n",
    "#            plt.show()\n",
    "             ## ENd mehreen add for view   \n",
    "                \n",
    "            \n",
    "            output_file = os.path.join(output_directory, \n",
    "                          basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            warp_output_file = os.path.join(output_directory, basename, \"{}-{}.png\".format(basename, line))\n",
    "            warp_output_file_save = os.path.join(basename, \"{}-{}.png\".format(basename, str(len(region_output_data))))\n",
    "            save_file = os.path.join(basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            region_output_data.append({\n",
    "                \"gt\": para_df.ground_truth[ind],\n",
    "                \"image_path\": save_file,\n",
    "                \"sol\": line_points[0],\n",
    "                \"lf\": line_points,\n",
    "                \"hw_path\": warp_output_file #MEhreen commentwarp_output_file_save\n",
    "            })\n",
    "            #print('****', output_file)\n",
    "            if not os.path.exists(os.path.dirname(output_file)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(output_file))\n",
    "                except OSError as exc:\n",
    "                    raise Exception(\"Could not write file\")\n",
    "\n",
    "            cv2.imwrite(warp_output_file, warped)\n",
    "\n",
    "        output_data.extend(region_output_data)\n",
    "\n",
    "    \n",
    "    if len(region_output_data) == 0:\n",
    "        return ''\n",
    "        \n",
    "    output_data_path =os.path.join(output_directory, basename, \"{}.json\".format(basename))\n",
    "    if not os.path.exists(os.path.dirname(output_data_path)):\n",
    "        os.makedirs(os.path.dirname(output_data_path))\n",
    "\n",
    "    with open(output_data_path, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "\n",
    "    return output_data_path    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3700d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_int_tuples(df_col):\n",
    "    tmp_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        if not pd.isna(item):\n",
    "            item = eval(item)    \n",
    "            tmp_col.append([(round(x[0]), round(x[1])) for x in item])\n",
    "        else:\n",
    "            tmp_col.append([])\n",
    "    return tmp_col\n",
    "\n",
    "def rotate_polygon(p):\n",
    "    if len(p) == 8 or len(p) == 7:\n",
    "        poly = p[4:]\n",
    "        poly.extend(p[0:4])\n",
    "        return poly\n",
    "    if len(p) == 4:\n",
    "        poly = [p[2], p[3], p[0], p[1]]\n",
    "        return poly\n",
    "    else:\n",
    "        print(\"something wrong\", p)\n",
    "        return []\n",
    "    \n",
    "def rotate_poly_list(df_col):\n",
    "    poly_list = [rotate_polygon(p) for p in df_col]\n",
    "    return poly_list\n",
    "\n",
    "\n",
    "def rotate_baseline_list(df_col):\n",
    "    b_list = [b[::-1] for b in df_col]\n",
    "    return b_list\n",
    "    \n",
    "# Reverse is true for right to left reading order    \n",
    "def remove_duplicate_baseline(baseline, reverse):\n",
    "    baseline.sort(key=lambda x: x[0], reverse=reverse)\n",
    "    unique_pts = [baseline[0]]\n",
    "    for pt in baseline[1:]:\n",
    "        if pt != unique_pts[-1]:\n",
    "            unique_pts.append(pt)\n",
    "    \n",
    "    return unique_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374673a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_img_dim(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    ht, width = img.shape[:2]\n",
    "    return (ht, width)\n",
    "\n",
    "# Subtract all x-coord from img_width as for Arabic we flip the image horizontally \n",
    "# Right to left reading order\n",
    "# Set reverse=True for baseline so that reading order becomes left to righ\n",
    "def flip_x_coord(df_col, img_width, reverse=False):\n",
    "    new_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        flipped = [(img_width-x, y) for (x,y) in item]\n",
    "        if reverse:\n",
    "            flipped = flipped[::-1]\n",
    "        new_col.append(flipped)\n",
    "    return new_col\n",
    "\n",
    "def process_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    DO_NOT_USE_WITHOUT_FLIPPING\n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df.reset_index(inplace=True)\n",
    "        valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        head, tail = os.path.split(para_df.image_file.iloc[0])\n",
    "        basename = 'base_' + tail\n",
    "        basename = basename[:-4]\n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        \n",
    "        json_path = handle_single_para(para_df, out_path+set_name)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "\n",
    "def process_notdone_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    \n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    \n",
    "    \n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        \n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        \n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.baseline = [remove_duplicate_baseline(b, reverse=False) for b in para_df.baseline]\n",
    "        \n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "\n",
    "        valid_region = False\n",
    "        if not pd.isna(para_df['region_type'][0]):\n",
    "            valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        \n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        basename = get_basename(img_path)\n",
    "        \n",
    "        tmp_path = os.path.join(out_path, set_name, basename, basename + '.json')\n",
    "        \n",
    "        #print('...', tmp_path)\n",
    "        if os.path.isfile(tmp_path):\n",
    "            json_path = tmp_path\n",
    "            print('... Done', basename)\n",
    "        else: \n",
    "            print('....Doing', basename)\n",
    "            json_path = handle_single_para(para_df, out_path+set_name, flip=True)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            print('Not added', basename)\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "def main_arabic_preprocess():\n",
    "    \n",
    "    # Make sure there is a / at end of path\n",
    "    paths = [\n",
    "             '/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr/', \n",
    "             ]\n",
    "    # Previously For batch_01\n",
    "    #set_name = ['Train_new', 'Valid_new', 'Test_new']\n",
    "    # Now changed to Train_01, Valid_01, Test_01\n",
    "    # For batch_01 corresponding file folder is all_files\n",
    "    # For batch_02 corresponding file folder is all_files_02\n",
    "    # For batch_03\n",
    "    #set_name = ['all_01', 'all_02', 'all_03']\n",
    "    set_name = ['all_03']\n",
    "\n",
    "    all_train = []\n",
    "    all_valid = []\n",
    "    all_test = []\n",
    "    for set_to_use in set_name:\n",
    "        \n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file= os.path.join(paths[0], set_to_use +'.csv'))\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "\n",
    " \n",
    "# This routine is for MoiseK batch01, batch02 and batch03\n",
    "#main_arabic_preprocess()\n",
    "#print('Preprocessing done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac21dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in set all_01: 66\n",
      "....Doing base_AR51_008\n",
      ".... 18\n",
      "....Doing base_AR51_013\n",
      ".... 18\n",
      "....Doing base_AR51_062\n",
      ".... 14\n",
      "....Doing base_AR56_01_001\n",
      ".... 4\n",
      "....Doing base_AR56_01_002\n",
      ".... 4\n",
      "....Doing base_AR56_01_003_1\n",
      ".... 14\n",
      "....Doing base_AR56_01_003_2\n",
      ".... 11\n",
      "....Doing base_AR56_01_004\n",
      ".... 24\n",
      "....Doing base_AR56_01_005\n",
      ".... 20\n",
      "....Doing base_AR56_01_006\n",
      ".... 23\n",
      "....Doing base_AR56_01_007\n",
      ".... 21\n",
      "....Doing base_AR56_01_008\n",
      ".... 18\n",
      "....Doing base_AR56_01_009\n",
      ".... 20\n",
      "....Doing base_AR56_01_010\n",
      ".... 18\n",
      "....Doing base_AR56_01_011\n",
      ".... 19\n",
      "....Doing base_AR56_01_012\n",
      ".... 16\n",
      "....Doing base_AR56_01_013\n",
      ".... 22\n",
      "....Doing base_AR56_01_014\n",
      ".... 21\n",
      "....Doing base_AR56_01_015\n",
      ".... 20\n",
      "....Doing base_AR56_01_016\n",
      ".... 48\n",
      "....Doing base_AR56_01_017\n",
      ".... 22\n",
      "....Doing base_AR56_01_018\n",
      ".... 18\n",
      "....Doing base_AR56_01_019\n",
      ".... 21\n",
      "....Doing base_AR56_01_020\n",
      ".... 24\n",
      "....Doing base_AR56_01_021\n",
      ".... 18\n",
      "....Doing base_AR56_01_022\n",
      ".... 21\n",
      "....Doing base_AR56_01_023\n",
      ".... 18\n",
      "....Doing base_AR56_01_024\n",
      ".... 20\n",
      "....Doing base_AR67_002\n",
      ".... 20\n",
      "....Doing base_AR67_003\n",
      ".... 11\n",
      "....Doing base_AR67_004\n",
      ".... 15\n",
      "....Doing base_AR69_003\n",
      ".... 12\n",
      "....Doing base_AR69_004\n",
      ".... 32\n",
      "....Doing base_AR69_005\n",
      ".... 20\n",
      "....Doing base_AR69_006\n",
      ".... 36\n",
      "....Doing base_AR69_007\n",
      ".... 29\n",
      "....Doing base_Baddour_ArabicLetter_1\n",
      ".... 11\n",
      "....Doing base_Baddour_ArabicLetter_2\n",
      ".... 18\n",
      "....Doing base_Baddour_Letter-1\n",
      ".... 20\n",
      "....Doing base_KEllis2020-137b\n",
      ".... 4\n",
      "....Doing base_KEllis2020-138b\n",
      ".... 6\n",
      "....Doing base_KEllis2020-140d\n",
      ".... 18\n",
      "....Doing base_KEllis2020-147b\n",
      ".... 8\n",
      "....Doing base_KEllis2020-173a\n",
      ".... 18\n",
      "....Doing base_Oussani2018-0282a\n",
      ".... 6\n",
      "....Doing base_Oussani2018-0283a\n",
      ".... 4\n",
      "....Doing base_Oussani2018-0284\n",
      ".... 21\n",
      "....Doing base_Oussani2018-0285\n",
      ".... 14\n",
      "....Doing base_Oussani2018-0286_1\n",
      ".... 28\n",
      "....Doing base_Oussani2018-0286_2\n",
      ".... 27\n",
      "....Doing base_Oussani2018-0292a_1\n",
      ".... 23\n",
      "....Doing base_Oussani2018-0292a_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292b_1\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292b_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292c_1\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292c_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292d_1\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292d_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292e_1\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292e_2\n",
      ".... 23\n",
      "....Doing base_Oussani2018-0292f_1\n",
      ".... 23\n",
      "....Doing base_Oussani2018-0292f_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292g_1\n",
      ".... 21\n",
      "....Doing base_Oussani2018-0292g_2\n",
      ".... 22\n",
      "....Doing base_Oussani2018-0292h_1\n",
      ".... 21\n",
      "....Doing base_Oussani2018-0292h_2\n",
      ".... 20\n",
      "Total files in set all_02: 50\n",
      "....Doing base_kc0061_01_01_19121203_001a\n",
      ".... 18\n",
      "....Doing base_kc0061_01_01_19121203_001b\n",
      ".... 11\n",
      "....Doing base_kc0061_01_02_19151008_001a\n",
      ".... 17\n",
      "....Doing base_kc0061_01_02_19151008_001b\n",
      ".... 10\n",
      "....Doing base_kc0061_01_03_19201107_001_1\n",
      ".... 19\n",
      "....Doing base_kc0061_01_03_19201107_001_2\n",
      ".... 22\n",
      "....Doing base_kc0061_01_03_19201109_001\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201109_002\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201109_003\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201109_004\n",
      ".... 9\n",
      "....Doing base_kc0061_01_03_19201111_001\n",
      ".... 20\n",
      "....Doing base_kc0061_01_03_19201111_002\n",
      ".... 22\n",
      "....Doing base_kc0061_01_03_19201111_003\n",
      ".... 19\n",
      "....Doing base_kc0061_01_03_19201113_001\n",
      ".... 20\n",
      "....Doing base_kc0061_01_03_19201113_002\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201113_003\n",
      ".... 24\n",
      "....Doing base_kc0061_01_03_19201119_001\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201119_002\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201119_003\n",
      ".... 5\n",
      "....Doing base_kc0061_01_03_19201127_001\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201200_001a\n",
      ".... 28\n",
      "....Doing base_kc0061_01_04_19210106_001\n",
      ".... 9\n",
      "....Doing base_kc0061_01_04_19210106_002\n",
      ".... 16\n",
      "....Doing base_kc0061_01_04_19210106_003\n",
      ".... 14\n",
      "....Doing base_kc0061_01_04_19210106_004\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210128_001\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210209_001a\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210209_001b\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_001\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210223_002\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_003\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_004\n",
      ".... 12\n",
      "....Doing base_kc0061_01_04_19210316_001\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210316_002\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210316_003\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210316_004\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210402_001a\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210402_001b\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210406_001\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210412_001a\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210412_001b\n",
      ".... 7\n",
      "....Doing base_kc0061_01_04_19210417_001\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210419_001a\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210419_001b\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210504_001a\n",
      ".... 16\n",
      "....Doing base_kc0061_01_04_19210504_001b\n",
      ".... 20\n",
      "....Doing base_kc0061_01_04_19210512_001\n",
      ".... 22\n",
      "....Doing base_kc0061_01_04_19210512_002\n",
      ".... 14\n",
      "....Doing base_kc0061_01_04_19210513_001\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210526_001\n",
      ".... 17\n",
      "Total files in set all_03: 65\n",
      "....Doing base_AR53_02_205\n",
      ".... 26\n",
      "....Doing base_AR53_02_207\n",
      ".... 26\n",
      "....Doing base_AR53_02_209\n",
      ".... 25\n",
      "....Doing base_AR53_02_210\n",
      ".... 24\n",
      "....Doing base_AR53_02_211\n",
      ".... 24\n",
      "....Doing base_AR53_02_212\n",
      ".... 24\n",
      "....Doing base_AR53_02_213\n",
      ".... 23\n",
      "....Doing base_AR53_02_230\n",
      ".... 26\n",
      "....Doing base_AR53_02_246\n",
      ".... 26\n",
      "....Doing base_AR53_02_254\n",
      ".... 24\n",
      "....Doing base_AR53_02_262\n",
      ".... 26\n",
      "....Doing base_AR53_02_265\n",
      ".... 25\n",
      "....Doing base_KJoseph19020905-001\n",
      ".... 23\n",
      "....Doing base_KJoseph19020905-002\n",
      ".... 30\n",
      "....Doing base_KJoseph19030207-001\n",
      ".... 20\n",
      "....Doing base_KJoseph19040209-001\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-001\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-002\n",
      ".... 24\n",
      "....Doing base_KJoseph19040819-001\n",
      ".... 20\n",
      "....Doing base_KJoseph19040819-002\n",
      ".... 26\n",
      "....Doing base_KJoseph19040819-003\n",
      ".... 21\n",
      "....Doing base_KJoseph19040819-004\n",
      ".... 27\n",
      "....Doing base_KJoseph19050928-002\n",
      ".... 27\n",
      "....Doing base_KJoseph19080319-001\n",
      ".... 21\n",
      "....Doing base_Nasrallah-Al001-034b\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034d\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034e\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034f\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034h\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034i\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034j\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034k\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034l\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034m\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034n\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034p\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034q\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-035a\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-035b\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-035c\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046a\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046b\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-077\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-103\n",
      ".... 28\n",
      "....Doing base_TAttallah2020-107\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-122a\n",
      ".... 31\n",
      "....Doing base_TAttallah2020-125a\n",
      ".... 25\n",
      "....Doing base_TAttallah2020-128c\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-130\n",
      ".... 23\n",
      "....Doing base_TAttallah2020-132a\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-132b\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-134\n",
      ".... 26\n",
      "....Doing base_kc0066_01_01_1873diary_061\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_062\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_063\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_064\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_065\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_164\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_165\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_166\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_167\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_168\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_169\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_170\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_171\n",
      ".... 13\n",
      "Total files in set all_04: 149\n",
      "....Doing base_2015 5-03 El-Khouri_Letter to Jennie Jabaley from Lebanon Oct18 1960_1\n",
      ".... 20\n",
      "....Doing base_2015 5-07 El-Khouri_Letter to Joseph from Lebanon Dec17 1959_1\n",
      ".... 21\n",
      "Not doing line 1\n",
      "Not doing line 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Doing base_2015 5-07b El-Khouri_Letter to Joseph from Lebanon Dec17 1959_2_1\n",
      ".... 16\n",
      "....Doing base_2015 5-07b El-Khouri_Letter to Joseph from Lebanon Dec17 1959_2_2\n",
      ".... 13\n",
      "....Doing base_2015 5-08b El-Khouri_Letter to Joseph from Lebanon Dec19 1957_2_1\n",
      ".... 19\n",
      "....Doing base_2015 5-08b El-Khouri_Letter to Joseph from Lebanon Dec19 1957_2_2\n",
      ".... 20\n",
      "....Doing base_2015 5-08c El-Khouri_Letter to Joseph from Lebanon Dec19 1957_3\n",
      ".... 18\n",
      "....Doing base_2015 5-09b El-Khouri_Letter to Joseph from Lebanon Dec21 1960_2\n",
      ".... 20\n",
      "....Doing base_2015 5-13 El-Khouri_Letter to Joseph from Lebanon Feb29 1960 1\n",
      ".... 18\n",
      "....Doing base_2015 5-13b El-Khouri_Letter to Joseph from Lebanon Feb29 1960_2_1\n",
      ".... 19\n",
      "....Doing base_AR53_01_006\n",
      ".... 19\n",
      "....Doing base_AR53_01_008\n",
      ".... 21\n",
      "....Doing base_AR53_01_009\n",
      ".... 24\n",
      "....Doing base_AR53_01_010\n",
      ".... 22\n",
      "....Doing base_AR53_01_011\n",
      ".... 21\n",
      "....Doing base_AR56_02_006\n",
      ".... 18\n",
      "....Doing base_AR56_03_002\n",
      ".... 12\n",
      "....Doing base_AR56_03_003\n",
      ".... 15\n",
      "....Doing base_AR56_03_007\n",
      ".... 18\n",
      "....Doing base_AR56_03_008\n",
      ".... 17\n",
      "....Doing base_AR56_04_033\n",
      ".... 14\n",
      "....Doing base_AR56_04_035\n",
      ".... 18\n",
      "....Doing base_AR56_04_036\n",
      ".... 21\n",
      "....Doing base_KEllis2018-139a\n",
      ".... 22\n",
      "....Doing base_KEllis2018-139b\n",
      ".... 22\n",
      "....Doing base_KEllis2018-139c\n",
      ".... 21\n",
      "....Doing base_KEllis2018-139d\n",
      ".... 24\n",
      "....Doing base_KEllis2018-139e\n",
      ".... 24\n",
      "....Doing base_KEllis2018-147e\n",
      ".... 18\n",
      "....Doing base_KEllis2018-147f\n",
      ".... 16\n",
      "....Doing base_KEllis2018-166a\n",
      ".... 24\n",
      "....Doing base_KEllis2018-166b\n",
      ".... 25\n",
      "....Doing base_KEllis2018-166c\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169a_1\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169a_2\n",
      ".... 15\n",
      "....Doing base_KEllis2018-169b_1\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169b_2\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169c_1\n",
      ".... 14\n",
      "....Doing base_KEllis2018-169c_2\n",
      ".... 14\n",
      "....Doing base_KEllis2018-169d_1\n",
      ".... 16\n",
      "....Doing base_KEllis2018-169d_2\n",
      ".... 16\n",
      "....Doing base_KEllis2018-175a\n",
      ".... 18\n",
      "....Doing base_KEllis2018-175b\n",
      ".... 17\n",
      "....Doing base_KEllis2018-193a\n",
      ".... 20\n",
      "....Doing base_KEllis2018-193b\n",
      ".... 21\n",
      "....Doing base_KEllis2018-193c\n",
      ".... 16\n",
      "....Doing base_KEllis2019-105a\n",
      ".... 10\n",
      "....Doing base_KEllis2019-106a\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-010a\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-010b\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010c\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-010d\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010e\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010f\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010g\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010h\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010i\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010j\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010k\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010l\n",
      ".... 11\n",
      "....Doing base_Nasrallah-Al001-010m\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010n\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-011a\n",
      ".... 11\n",
      "....Doing base_Nasrallah-Al001-013b\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-013c\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013d\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013e\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013f\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013g\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014b\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014c\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014d\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014e\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014f\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014g\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014h\n",
      ".... 14\n",
      "....Doing base_TAttallah2020-019\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-020\n",
      ".... 15\n",
      "....Doing base_TAttallah2020-023\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-024\n",
      ".... 21\n",
      "....Doing base_TAttallah2020-026\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-027a\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-027b\n",
      ".... 18\n",
      "....Doing base_TAttallah2020-029\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-031a\n",
      ".... 10\n",
      "....Doing base_TAttallah2020-033a\n",
      ".... 20\n",
      "....Doing base_TAttallah2020-034\n",
      ".... 25\n",
      "....Doing base_TAttallah2020-035\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-037\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-038\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-039\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-094a_resize\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-095_resize\n",
      ".... 24\n",
      "....Doing base_TAttallah2020-099a_resize\n",
      ".... 29\n",
      "....Doing base_TAttallah2020-105_resize\n",
      ".... 29\n",
      "....Doing base_TAttallah2020-107_resize\n",
      ".... 27\n",
      "....Doing base_kc0066_01_01_1873diary_005\n",
      ".... 12\n",
      "....Doing base_kc0066_01_01_1873diary_006\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_007\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_008\n",
      ".... 14\n",
      "done 100\n",
      "....Doing base_kc0066_01_01_1873diary_009\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_010\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_011\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_012\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_013\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_014\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_015\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_016\n",
      ".... 9\n",
      "....Doing base_kc0066_01_01_1873diary_017\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_018\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_019\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_020\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_021\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_022\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_025\n",
      ".... 11\n",
      "....Doing base_kc0066_01_01_1873diary_026\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_027\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_028\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_029\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_030\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_031\n",
      ".... 14\n",
      "Not doing line 1\n",
      "....Doing base_kc0066_01_01_1873diary_032\n",
      ".... 9\n",
      "....Doing base_kc0066_01_01_1873diary_033\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_034\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_035\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_036\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_037\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_038\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_039\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_040\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_041\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_042\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_043\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_044\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_045\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_046\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_047\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_048\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_049\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_050\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_051\n",
      ".... 12\n",
      "....Doing base_kc0066_01_01_1873diary_052\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_053\n",
      ".... 11\n",
      "....Doing base_kc0066_01_01_1873diary_054\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_055\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_056\n",
      ".... 10\n",
      "....Doing base_kc0066_01_01_1873diary_057\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_058\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_059\n",
      ".... 15\n",
      "Total files in set all_07: 67\n",
      "....Doing base_26025_01r\n",
      ".... 23\n",
      "....Doing base_26025_08\n",
      ".... 18\n",
      "....Doing base_26025_09\n",
      ".... 22\n",
      "....Doing base_26066_05\n",
      ".... 18\n",
      "....Doing base_26066_06r\n",
      ".... 18\n",
      "....Doing base_26066_06v\n",
      ".... 13\n",
      "....Doing base_27404_2_138\n",
      ".... 18\n",
      "....Doing base_27404_2_139\n",
      ".... 18\n",
      "....Doing base_27404_2_140\n",
      ".... 14\n",
      "....Doing base_27404_2_141\n",
      ".... 18\n",
      "....Doing base_27404_2_142\n",
      ".... 19\n",
      "....Doing base_27404_2_143\n",
      ".... 19\n",
      "....Doing base_27404_2_144\n",
      ".... 19\n",
      "....Doing base_27404_2_155\n",
      ".... 19\n",
      "....Doing base_27404_2_163\n",
      ".... 21\n",
      "....Doing base_27404_2_164\n",
      ".... 20\n",
      "....Doing base_27404_2_165\n",
      ".... 19\n",
      "....Doing base_27404_2_166\n",
      ".... 21\n",
      "....Doing base_27404_2_167\n",
      ".... 21\n",
      "....Doing base_27404_2_168\n",
      ".... 21\n",
      "....Doing base_27404_2_170\n",
      ".... 21\n",
      "....Doing base_JEH2A1_01r\n",
      ".... 20\n",
      "....Doing base_JEH2A2_17\n",
      ".... 22\n",
      "....Doing base_JEH2A2_18\n",
      ".... 30\n",
      "....Doing base_JEH2A2_19\n",
      ".... 25\n",
      "....Doing base_JEH2A2_20_01\n",
      ".... 28\n",
      "....Doing base_JEH2A2_20_02\n",
      ".... 19\n",
      "....Doing base_JEH2E1_19r\n",
      ".... 18\n",
      "....Doing base_JEH2E1_20_04r\n",
      ".... 18\n",
      "....Doing base_JEH2E1_21_01r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_21_02r\n",
      ".... 15\n",
      "....Doing base_JEH2E1_22r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_23r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_24r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_26r\n",
      ".... 16\n",
      "....Doing base_JEH2E1_27r\n",
      ".... 17\n",
      "....Doing base_JEH2E1_30_08r\n",
      ".... 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Doing base_JEH2E1_30_10r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_30_13r\n",
      ".... 20\n",
      "....Doing base_JEH2E1_30_15r\n",
      ".... 21\n",
      "....Doing base_JEH2E1_41_01r\n",
      ".... 17\n",
      "....Doing base_JEH2E1_59_06r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_59_07r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_59_28r\n",
      ".... 15\n",
      "....Doing base_JEH2E1_61_08r\n",
      ".... 14\n",
      "....Doing base_JEH2E1_90_03r\n",
      ".... 17\n",
      "....Doing base_JEH2F1_30_01r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_01v\n",
      ".... 19\n",
      "....Doing base_JEH2F1_30_02r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_02v\n",
      ".... 25\n",
      "....Doing base_JEH2F1_30_03r\n",
      ".... 24\n",
      "....Doing base_JEH2F1_30_03v\n",
      ".... 23\n",
      "....Doing base_JEH2F1_30_04r\n",
      ".... 27\n",
      "....Doing base_JEH2F1_30_04v\n",
      ".... 25\n",
      "....Doing base_JEH2F1_30_05\n",
      ".... 18\n",
      "....Doing base_ME_21_01\n",
      ".... 27\n",
      "....Doing base_ME_21_02\n",
      ".... 26\n",
      "....Doing base_ME_21_03\n",
      ".... 29\n",
      "....Doing base_ME_21_04\n",
      ".... 28\n",
      "....Doing base_ME_21_05\n",
      ".... 29\n",
      "....Doing base_ME_21_06\n",
      ".... 26\n",
      "....Doing base_ME_21_07\n",
      ".... 29\n",
      "....Doing base_ME_21_08\n",
      ".... 29\n",
      "....Doing base_ME_21_09\n",
      ".... 28\n",
      "....Doing base_ME_21_10\n",
      ".... 27\n",
      "....Doing base_ME_21_11\n",
      ".... 30\n",
      "....Doing base_ME_21_12\n",
      ".... 19\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# This routine is for scribe arabic\n",
    "def preprocess_scribe():\n",
    "    # Make sure there is a / at end of path\n",
    "    paths = ['/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr/']\n",
    "    set_name = ['all_01',  'all_02',   'all_03',   'all_04',   'all_07']\n",
    "    batches = ['batch_01', 'batch_02', 'batch_03', 'batch_04', 'batch_07']\n",
    "    input_file_dir = '/home/msaeed3/mehreen/datasets/scribeArabic/' \n",
    "    \n",
    "    for set_to_use, batch in zip(set_name, batches):\n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file=input_file_dir+f'{batch}.csv')\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "            \n",
    "preprocess_scribe()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4dba9",
   "metadata": {},
   "source": [
    "# Uncomment this cell to check for a single file\n",
    "\n",
    "out_path = '/home/msaeed3/mehreen/datasets/MoiseK/'\n",
    "\n",
    "check_file = '15805_01r_cropped.jpg' \n",
    "set_name = 'all_05'\n",
    "csv_file = '/home/msaeed3/mehreen/datasets/scribeArabic/batch_05.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "\n",
    "df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "\n",
    "files = df.image_file\n",
    "\n",
    "all_ground_truth = []\n",
    "for img_path in set(files):\n",
    "    \n",
    "    if check_file in img_path:\n",
    "        print('***')\n",
    "        para_df = df[df.image_file == img_path]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "        valid_region = any(s in para_df['region_type'][0] for s in ['paragraph','text'])\n",
    "        if not valid_region:\n",
    "            print('NOT DONE')\n",
    "        json_path = handle_single_para(para_df, out_path+set_name, flip=True)       \n",
    "        print(json_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16415cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpara_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'para_df' is not defined"
     ]
    }
   ],
   "source": [
    "para_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19efa",
   "metadata": {},
   "source": [
    "# This is for priinting the json file in readable format\n",
    "\n",
    "json_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train.json'\n",
    "with open(json_file, 'r') as fin:\n",
    "    json_obj = json.load(fin)\n",
    "\n",
    "out_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train_readable.json'\n",
    "with open(out_file, 'w') as fout:\n",
    "    json.dump(json_obj, fout, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
