{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import parse_PAGE\n",
    "import cv2\n",
    "import line_extraction\n",
    "import numpy as np\n",
    "import os\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from scipy import ndimage\n",
    "import json\n",
    "import codecs\n",
    "from svgpathtools import Path, Line\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "HT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6462894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_offset_mapping(img, ts, path, offset_1, offset_2, max_min = None, cube_size = None):\n",
    "    # cube_size = 80\n",
    "\n",
    "    offset_1_pts = []\n",
    "    offset_2_pts = []\n",
    "    # for t in ts:\n",
    "    for i in range(len(ts)):\n",
    "        t = ts[i]\n",
    "        pt = path.point(t)\n",
    "\n",
    "        norm = None\n",
    "        if i == 0:\n",
    "            norm = normal(pt, path.point(ts[i+1]))\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        elif i == len(ts)-1:\n",
    "            norm = normal(path.point(ts[i-1]), pt)\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "        else:\n",
    "            norm1 = normal(path.point(ts[i-1]), pt)\n",
    "            norm1 = norm1 / dis(complex(0,0), norm1)\n",
    "            norm2 = normal(pt, path.point(ts[i+1]))\n",
    "            norm2 = norm2 / dis(complex(0,0), norm2)\n",
    "\n",
    "            norm = (norm1 + norm2)/2\n",
    "            norm = norm / dis(complex(0,0), norm)\n",
    "\n",
    "        offset_vector1 = offset_1 * norm\n",
    "        offset_vector2 = offset_2 * norm\n",
    "\n",
    "        pt1 = pt + offset_vector1\n",
    "        pt2 = pt + offset_vector2\n",
    "\n",
    "        offset_1_pts.append(complexToNpPt(pt1))\n",
    "        offset_2_pts.append(complexToNpPt(pt2))\n",
    "\n",
    "    offset_1_pts = np.array(offset_1_pts)\n",
    "    offset_2_pts = np.array(offset_2_pts)\n",
    "\n",
    "    h,w = img.shape[:2]\n",
    "\n",
    "    offset_source2 = np.array([(cube_size*i, 0) for i in range(len(offset_1_pts))], dtype=np.float32)\n",
    "    offset_source1 = np.array([(cube_size*i, cube_size) for i in range(len(offset_2_pts))], dtype=np.float32)\n",
    "\n",
    "    offset_source1 = offset_source1[::-1]\n",
    "    offset_source2 = offset_source2[::-1]\n",
    "\n",
    "    source = np.concatenate([offset_source1, offset_source2])\n",
    "    destination = np.concatenate([offset_1_pts, offset_2_pts])\n",
    "\n",
    "    source = source[:,::-1]\n",
    "    destination = destination[:,::-1]\n",
    "\n",
    "    n_w = int(offset_source2[:,0].max())\n",
    "    n_h = int(cube_size)\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:n_h, 0:n_w]\n",
    "\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(n_h,n_w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    rectified_to_warped_x = map_x_32\n",
    "    rectified_to_warped_y = map_y_32\n",
    "\n",
    "    grid_x, grid_y = np.mgrid[0:h, 0:w]\n",
    "    grid_z = griddata(source, destination, (grid_x, grid_y), method='cubic')\n",
    "    map_x = np.append([], [ar[:,1] for ar in grid_z]).reshape(h,w)\n",
    "    map_y = np.append([], [ar[:,0] for ar in grid_z]).reshape(h,w)\n",
    "    map_x_32 = map_x.astype('float32')\n",
    "    map_y_32 = map_y.astype('float32')\n",
    "\n",
    "    warped_to_rectified_x = map_x_32\n",
    "    warped_to_rectified_y = map_y_32\n",
    "\n",
    "    return rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min\n",
    "\n",
    "\n",
    "def dis(pt1, pt2):\n",
    "    a = (pt1.real - pt2.real)**2\n",
    "    b = (pt1.imag - pt2.imag)**2\n",
    "    return np.sqrt(a+b)\n",
    "\n",
    "def complexToNpPt(pt):\n",
    "    return np.array([pt.real, pt.imag], dtype=np.float32)\n",
    "\n",
    "def normal(pt1, pt2):\n",
    "    dif = pt1 - pt2\n",
    "    return complex(-dif.imag, dif.real)\n",
    "\n",
    "def find_t_spacing(path, cube_size):\n",
    "\n",
    "    l = path.length()\n",
    "    error = 0.01\n",
    "    init_step_size = cube_size / l\n",
    "\n",
    "    last_t = 0\n",
    "    cur_t = 0\n",
    "    pts = []\n",
    "    ts = [0]\n",
    "    pts.append(complexToNpPt(path.point(cur_t)))\n",
    "    path_lookup = {}\n",
    "    for target in np.arange(cube_size, int(l), cube_size):\n",
    "        step_size = init_step_size\n",
    "        for i in range(1000):\n",
    "            cur_length = dis(path.point(last_t), path.point(cur_t))\n",
    "            if np.abs(cur_length - cube_size) < error:\n",
    "                break\n",
    "\n",
    "            step_t = min(cur_t + step_size, 1.0)\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_t = max(cur_t - step_size, 0.0)\n",
    "            step_t = max(step_t, last_t)\n",
    "            step_t = max(step_t, 1.0)\n",
    "\n",
    "            step_l = dis(path.point(last_t), path.point(step_t))\n",
    "\n",
    "            if np.abs(step_l - cube_size) < np.abs(cur_length - cube_size):\n",
    "                cur_t = step_t\n",
    "                continue\n",
    "\n",
    "            step_size = step_size / 2.0\n",
    "\n",
    "        last_t = cur_t\n",
    "\n",
    "        ts.append(cur_t)\n",
    "        pts.append(complexToNpPt(path.point(cur_t)))\n",
    "\n",
    "    pts = np.array(pts)\n",
    "\n",
    "    return ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6373c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbd6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basename(img_name):\n",
    "    head, tail = os.path.split(img_name)\n",
    "    basename = 'base_' + tail\n",
    "    basename = basename[:-4]\n",
    "    return basename\n",
    "\n",
    "def handle_single_para(para_df, output_directory, flip=False):\n",
    "    \n",
    "    output_data = []\n",
    "    num_lines = len(para_df)\n",
    "    print('....', num_lines)\n",
    "    if os.path.exists(para_df.image_file.iloc[0]):\n",
    "        img = cv2.imread(para_df.image_file.iloc[0])\n",
    "        if flip:\n",
    "            img = cv2.flip(img, 1)\n",
    "    else:\n",
    "        print('....File not found', para_df.image_file.iloc[0])\n",
    "        return ''\n",
    "    basename = get_basename(para_df.image_file.iloc[0])    \n",
    "    \n",
    "    all_lines = \"\"\n",
    "    \n",
    "    # get rid of png/jpg extension\n",
    "    \n",
    "    for region in [0]:\n",
    "        region_output_data = []\n",
    "        #print('in region', region)\n",
    "        for ind, line in enumerate(para_df.line_number):\n",
    "            if len(para_df.polygon_pts[ind]) == 0:\n",
    "                print('No polygon pts in img', para_df.image_file.iloc[0][-15:],\n",
    "                      'line number', line)\n",
    "                continue\n",
    "            #print('....ind, line', ind, line)\n",
    "            line_mask = line_extraction.extract_region_mask(img, para_df.polygon_pts[ind])\n",
    "            masked_img = img.copy()\n",
    "            masked_img[line_mask==0] = 0\n",
    "\n",
    "            summed_axis0 = (masked_img.astype(float) / 255).sum(axis=0)\n",
    "            summed_axis1 = (masked_img.astype(float) / 255).sum(axis=1)\n",
    "\n",
    "            non_zero_cnt0 = np.count_nonzero(summed_axis0) / float(len(summed_axis0))\n",
    "            non_zero_cnt1 = np.count_nonzero(summed_axis1) / float(len(summed_axis1))\n",
    "\n",
    "            avg_height0 = np.median(summed_axis0[summed_axis0 != 0])\n",
    "            avg_height1 = np.median(summed_axis1[summed_axis1 != 0])\n",
    "\n",
    "            avg_height = min(avg_height0, avg_height1)\n",
    "            if non_zero_cnt0 > non_zero_cnt1:\n",
    "                target_step_size = avg_height0\n",
    "            else:\n",
    "                target_step_size = avg_height1\n",
    "\n",
    "            paths = []\n",
    "            for i in range(len(para_df.baseline[ind])-1):\n",
    "                i_1 = i+1\n",
    "\n",
    "                p1 = para_df.baseline[ind][i]\n",
    "                p2 = para_df.baseline[ind][i_1]\n",
    "\n",
    "                p1_c = complex(*p1)\n",
    "                p2_c = complex(*p2)\n",
    "\n",
    "\n",
    "                paths.append(Line(p1_c, p2_c))\n",
    "\n",
    "\n",
    "            # Add a bit on the end\n",
    "            tan = paths[-1].unit_tangent(1.0)\n",
    "            p3_c = p2_c + target_step_size * tan\n",
    "            paths.append(Line(p2_c, p3_c))\n",
    "\n",
    "            path = Path(*paths)\n",
    "            \n",
    "            try:\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                \n",
    "                #Changing this causes issues in pretraining - not sure why\n",
    "                target_height = HT\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 0, -2*target_step_size, cube_size = target_height)\n",
    "                warped_above = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, 2*target_step_size, 0, cube_size = target_height)\n",
    "                warped_below = cv2.remap(line_mask, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(0,0,0))\n",
    "\n",
    "                above_scale =  np.max((warped_above.astype(float) / 255).sum(axis=0))\n",
    "                below_scale = np.max((warped_below.astype(float) / 255).sum(axis=0))\n",
    "\n",
    "\n",
    "                \n",
    "                ab_sum = above_scale + below_scale\n",
    "                above = target_step_size * (above_scale/ab_sum)\n",
    "                below = target_step_size * (below_scale/ab_sum)\n",
    "\n",
    "                above = target_step_size * (above_scale/(target_height/2.0))\n",
    "                below = target_step_size * (below_scale/(target_height/2.0))\n",
    "                target_step_size = above + below\n",
    "                ts = find_t_spacing(path, target_step_size)\n",
    "                if len(ts) <= 1:\n",
    "                    print('Not doing line', line)\n",
    "                    continue\n",
    "                rectified_to_warped_x, rectified_to_warped_y, warped_to_rectified_x, warped_to_rectified_y, max_min = generate_offset_mapping(masked_img, ts, path, below, -above, cube_size=target_height)\n",
    "\n",
    "                rectified_to_warped_x = rectified_to_warped_x[::-1,::-1]\n",
    "                rectified_to_warped_y = rectified_to_warped_y[::-1,::-1]\n",
    "\n",
    "                warped_to_rectified_x = warped_to_rectified_x[::-1,::-1]\n",
    "                warped_to_rectified_y = warped_to_rectified_y[::-1,::-1]\n",
    "\n",
    "                warped = cv2.remap(img, rectified_to_warped_x, rectified_to_warped_y, cv2.INTER_CUBIC, borderValue=(255,255,255))\n",
    "            except:\n",
    "                print('Not doing line', line)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            mapping = np.stack([rectified_to_warped_y, rectified_to_warped_x], axis=2)\n",
    "\n",
    "            top_left = mapping[0,0,:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "            btm_right = mapping[min(mapping.shape[0]-1, target_height-1), min(mapping.shape[1]-1, target_height-1),:] / np.array(img.shape[:2]).astype(np.float32)\n",
    "\n",
    "\n",
    "            line_points = []\n",
    "            for i in range(0,mapping.shape[1],target_height):\n",
    "\n",
    "                x0 = float(rectified_to_warped_x[0,i])\n",
    "                x1 = float(rectified_to_warped_x[-1,i])\n",
    "\n",
    "                y0 = float(rectified_to_warped_y[0,i])\n",
    "                y1 = float(rectified_to_warped_y[-1,i])\n",
    "\n",
    "                line_points.append({\n",
    "                    \"x0\": x0, \n",
    "                    \"x1\": x1, \n",
    "                    \"y0\": y0, \n",
    "                    \"y1\": y1, \n",
    "                })\n",
    "                \n",
    "                \n",
    "                                \n",
    "            ###Mehreen add for viewing\n",
    "\n",
    "#            plt.imshow(img) # or display line warped\n",
    "#            print(\"****\", line_points)\n",
    "#            for coord in line_points:\n",
    "#                x = coord[\"x0\"]\n",
    "#                y = coord[\"y0\"]\n",
    "#                x1 = coord[\"x1\"]\n",
    "#                y1 = coord[\"y1\"]\n",
    "                #rect = patches.Rectangle((x, y), np.abs(x-coord[2]), np.abs(y-coord[3]), facecolor='green')\n",
    "#                rect = patches.Rectangle((x, y), 10, 10, facecolor='blue')\n",
    "#                rect1 = patches.Rectangle((x1, y1), 10, 10, facecolor='red')\n",
    "#                plt.gca().add_patch(rect)  \n",
    "#                plt.gca().add_patch(rect1)\n",
    "#            rect0 = patches.Rectangle((line_points[0][\"x0\"], line_points[0][\"y0\"]), 10, 10, facecolor='yellow') \n",
    "#            plt.gca().add_patch(rect0)\n",
    "#            plt.show()\n",
    "             ## ENd mehreen add for view   \n",
    "                \n",
    "            \n",
    "            output_file = os.path.join(output_directory, \n",
    "                          basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            warp_output_file = os.path.join(output_directory, basename, \"{}-{}.png\".format(basename, line))\n",
    "            warp_output_file_save = os.path.join(basename, \"{}-{}.png\".format(basename, str(len(region_output_data))))\n",
    "            save_file = os.path.join(basename, \"{}~{}~{}.png\".format(basename, region, line))\n",
    "            region_output_data.append({\n",
    "                \"gt\": para_df.ground_truth[ind],\n",
    "                \"image_path\": save_file,\n",
    "                \"sol\": line_points[0],\n",
    "                \"lf\": line_points,\n",
    "                \"hw_path\": warp_output_file #MEhreen commentwarp_output_file_save\n",
    "            })\n",
    "            #print('****', output_file)\n",
    "            if not os.path.exists(os.path.dirname(output_file)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(output_file))\n",
    "                except OSError as exc:\n",
    "                    raise Exception(\"Could not write file\")\n",
    "\n",
    "            cv2.imwrite(warp_output_file, warped)\n",
    "\n",
    "        output_data.extend(region_output_data)\n",
    "\n",
    "    \n",
    "    if len(region_output_data) == 0:\n",
    "        return ''\n",
    "        \n",
    "    output_data_path =os.path.join(output_directory, basename, \"{}.json\".format(basename))\n",
    "    if not os.path.exists(os.path.dirname(output_data_path)):\n",
    "        os.makedirs(os.path.dirname(output_data_path))\n",
    "\n",
    "    with open(output_data_path, 'w') as f:\n",
    "        json.dump(output_data, f)\n",
    "\n",
    "    return output_data_path    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3700d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_int_tuples(df_col):\n",
    "    tmp_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        if not pd.isna(item):\n",
    "            item = eval(item)    \n",
    "            tmp_col.append([(round(x[0]), round(x[1])) for x in item])\n",
    "        else:\n",
    "            tmp_col.append([])\n",
    "    return tmp_col\n",
    "\n",
    "def rotate_polygon(p):\n",
    "    if len(p) == 8 or len(p) == 7:\n",
    "        poly = p[4:]\n",
    "        poly.extend(p[0:4])\n",
    "        return poly\n",
    "    if len(p) == 4:\n",
    "        poly = [p[2], p[3], p[0], p[1]]\n",
    "        return poly\n",
    "    else:\n",
    "        print(\"something wrong\", p)\n",
    "        return []\n",
    "    \n",
    "def rotate_poly_list(df_col):\n",
    "    poly_list = [rotate_polygon(p) for p in df_col]\n",
    "    return poly_list\n",
    "\n",
    "\n",
    "def rotate_baseline_list(df_col):\n",
    "    b_list = [b[::-1] for b in df_col]\n",
    "    return b_list\n",
    "    \n",
    "# Reverse is true for right to left reading order    \n",
    "def remove_duplicate_baseline(baseline, reverse):\n",
    "    baseline.sort(key=lambda x: x[0], reverse=reverse)\n",
    "    unique_pts = [baseline[0]]\n",
    "    for pt in baseline[1:]:\n",
    "        if pt != unique_pts[-1]:\n",
    "            unique_pts.append(pt)\n",
    "    \n",
    "    return unique_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374673a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_img_dim(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    ht, width = img.shape[:2]\n",
    "    return (ht, width)\n",
    "\n",
    "# Subtract all x-coord from img_width as for Arabic we flip the image horizontally \n",
    "# Right to left reading order\n",
    "# Set reverse=True for baseline so that reading order becomes left to righ\n",
    "def flip_x_coord(df_col, img_width, reverse=False):\n",
    "    new_col = []\n",
    "    for ind, item in enumerate(df_col):\n",
    "        flipped = [(img_width-x, y) for (x,y) in item]\n",
    "        if reverse:\n",
    "            flipped = flipped[::-1]\n",
    "        new_col.append(flipped)\n",
    "    return new_col\n",
    "\n",
    "def process_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    DO_NOT_USE_WITHOUT_FLIPPING\n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    #df.baseline = rotate_baseline_list(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    #df.polygon_pts = rotate_poly_list(df.polygon_pts) \n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df.reset_index(inplace=True)\n",
    "        valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        head, tail = os.path.split(para_df.image_file.iloc[0])\n",
    "        basename = 'base_' + tail\n",
    "        basename = basename[:-4]\n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        \n",
    "        json_path = handle_single_para(para_df, out_path+set_name)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "\n",
    "def process_notdone_arabic_dir(out_path, set_name, region_types, input_file=None):\n",
    "    \n",
    "    if input_file is None:\n",
    "        df = pd.read_csv(os.path.join(out_path, set_name +'.csv'))\n",
    "    else:\n",
    "        df = pd.read_csv(input_file)\n",
    "    \n",
    "    \n",
    "    df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "    df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "    \n",
    "    files = df.image_file\n",
    "    print(f'Total files in set {set_name}: {len(set(files))}')\n",
    "    \n",
    "    all_ground_truth = []\n",
    "    for para_numb in set(df.paragraph_number):\n",
    "        \n",
    "        para_df = df[df.paragraph_number == para_numb]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        \n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.baseline = [remove_duplicate_baseline(b, reverse=False) for b in para_df.baseline]\n",
    "        \n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "\n",
    "        valid_region = False\n",
    "        if not pd.isna(para_df['region_type'][0]):\n",
    "            valid_region = any(s in para_df['region_type'][0] for s in region_types)\n",
    "        if not valid_region:\n",
    "            continue\n",
    "        \n",
    "        img_path = para_df.image_file.iloc[0]\n",
    "        basename = get_basename(img_path)\n",
    "        \n",
    "        tmp_path = os.path.join(out_path, set_name, basename, basename + '.json')\n",
    "        \n",
    "        #print('...', tmp_path)\n",
    "        if os.path.isfile(tmp_path):\n",
    "            json_path = tmp_path\n",
    "            print('... Done', basename)\n",
    "        else: \n",
    "            print('....Doing', basename)\n",
    "            json_path = handle_single_para(para_df, out_path+set_name, flip=True)  \n",
    "            \n",
    "        # para not added    \n",
    "        if len(json_path) == 0:\n",
    "            print('Not added', basename)\n",
    "            continue\n",
    "            \n",
    "        all_ground_truth.append([json_path, img_path])\n",
    "        if len(all_ground_truth)%100 == 0:\n",
    "            print('done', len(all_ground_truth))\n",
    "        \n",
    "    return all_ground_truth\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "def main_arabic_preprocess():\n",
    "    \n",
    "    # Make sure there is a / at end of path\n",
    "    paths = [\n",
    "             '/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr/', \n",
    "             ]\n",
    "    # Previously For batch_01\n",
    "    #set_name = ['Train_new', 'Valid_new', 'Test_new']\n",
    "    # Now changed to Train_01, Valid_01, Test_01\n",
    "    # For batch_01 corresponding file folder is all_files\n",
    "    # For batch_02 corresponding file folder is all_files_02\n",
    "    # For batch_03\n",
    "    #set_name = ['all_01', 'all_02', 'all_03']\n",
    "    set_name = ['all_03']\n",
    "\n",
    "    all_train = []\n",
    "    all_valid = []\n",
    "    all_test = []\n",
    "    for set_to_use in set_name:\n",
    "        \n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file= os.path.join(paths[0], set_to_use +'.csv'))\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "\n",
    " \n",
    "# This routine is for MoiseK batch01, batch02 and batch03\n",
    "#main_arabic_preprocess()\n",
    "#print('Preprocessing done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac21dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in set all1: 50\n",
      "....Doing base_kc0061_01_01_19121203_001a_clean\n",
      ".... 18\n",
      "....Doing base_kc0061_01_01_19121203_001b_clean\n",
      ".... 11\n",
      "....Doing base_kc0061_01_02_19151008_001a_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_02_19151008_001b_clean\n",
      ".... 10\n",
      "....Doing base_kc0061_01_03_19201107_001_1_clean\n",
      ".... 19\n",
      "....Doing base_kc0061_01_03_19201107_001_2_clean\n",
      ".... 22\n",
      "....Doing base_kc0061_01_03_19201109_001_clean\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201109_002_clean\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201109_003_clean\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201109_004_clean\n",
      ".... 9\n",
      "....Doing base_kc0061_01_03_19201111_001_clean\n",
      ".... 20\n",
      "....Doing base_kc0061_01_03_19201111_002_clean\n",
      ".... 22\n",
      "....Doing base_kc0061_01_03_19201111_003_clean\n",
      ".... 19\n",
      "....Doing base_kc0061_01_03_19201113_001_clean\n",
      ".... 20\n",
      "....Doing base_kc0061_01_03_19201113_002_clean\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201113_003_clean\n",
      ".... 24\n",
      "....Doing base_kc0061_01_03_19201119_001_clean\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201119_002_clean\n",
      ".... 23\n",
      "....Doing base_kc0061_01_03_19201119_003_clean\n",
      ".... 5\n",
      "....Doing base_kc0061_01_03_19201127_001_clean\n",
      ".... 21\n",
      "....Doing base_kc0061_01_03_19201200_001a_clean\n",
      ".... 28\n",
      "....Doing base_kc0061_01_04_19210106_001_clean\n",
      ".... 9\n",
      "....Doing base_kc0061_01_04_19210106_002_clean\n",
      ".... 16\n",
      "....Doing base_kc0061_01_04_19210106_003_clean\n",
      ".... 14\n",
      "....Doing base_kc0061_01_04_19210106_004_clean\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210128_001_clean\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210209_001a_clean\n",
      ".... 13\n",
      "....Doing base_kc0061_01_04_19210209_001b_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_001_clean\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210223_002_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_003_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210223_004_clean\n",
      ".... 12\n",
      "....Doing base_kc0061_01_04_19210316_001_clean\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210316_002_clean\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210316_003_clean\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210316_004_clean\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210402_001a_clean\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210402_001b_clean\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210406_001_clean\n",
      ".... 18\n",
      "....Doing base_kc0061_01_04_19210412_001a_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210412_001b_clean\n",
      ".... 7\n",
      "....Doing base_kc0061_01_04_19210417_001_clean\n",
      ".... 17\n",
      "....Doing base_kc0061_01_04_19210419_001a_clean\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210419_001b_clean\n",
      ".... 19\n",
      "....Doing base_kc0061_01_04_19210504_001a_clean\n",
      ".... 16\n",
      "....Doing base_kc0061_01_04_19210504_001b_clean\n",
      ".... 20\n",
      "....Doing base_kc0061_01_04_19210512_001_clean\n",
      ".... 22\n",
      "....Doing base_kc0061_01_04_19210512_002_clean\n",
      ".... 14\n",
      "....Doing base_kc0061_01_04_19210513_001_clean\n",
      ".... 15\n",
      "....Doing base_kc0061_01_04_19210526_001_clean\n",
      ".... 17\n",
      "Total files in set all2: 65\n",
      "....Doing base_AR53_02_205_clean\n",
      ".... 26\n",
      "....Doing base_AR53_02_207_clean\n",
      ".... 26\n",
      "....Doing base_AR53_02_209_clean\n",
      ".... 25\n",
      "....Doing base_AR53_02_210_clean\n",
      ".... 24\n",
      "....Doing base_AR53_02_211_clean\n",
      ".... 24\n",
      "....Doing base_AR53_02_212_clean\n",
      ".... 24\n",
      "....Doing base_AR53_02_213_clean\n",
      ".... 23\n",
      "....Doing base_AR53_02_230_clean\n",
      ".... 26\n",
      "....Doing base_AR53_02_246_clean\n",
      ".... 26\n",
      "....Doing base_AR53_02_254_clean\n",
      ".... 24\n",
      "....Doing base_AR53_02_262_clean\n",
      ".... 26\n",
      "....Doing base_AR53_02_265_clean\n",
      ".... 25\n",
      "....Doing base_KJoseph19020905-001_clean\n",
      ".... 23\n",
      "....Doing base_KJoseph19020905-002_clean\n",
      ".... 30\n",
      "....Doing base_KJoseph19030207-001_clean\n",
      ".... 20\n",
      "....Doing base_KJoseph19040209-001_clean\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-001_clean\n",
      ".... 21\n",
      "....Doing base_KJoseph19040607-002_clean\n",
      ".... 24\n",
      "....Doing base_KJoseph19040819-001_clean\n",
      ".... 20\n",
      "....Doing base_KJoseph19040819-002_clean\n",
      ".... 26\n",
      "....Doing base_KJoseph19040819-003_clean\n",
      ".... 21\n",
      "....Doing base_KJoseph19040819-004_clean\n",
      ".... 27\n",
      "....Doing base_KJoseph19050928-002_clean\n",
      ".... 27\n",
      "....Doing base_KJoseph19080319-001_clean\n",
      ".... 21\n",
      "....Doing base_Nasrallah-Al001-034b_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034d_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034e_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034f_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034h_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034i_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034j_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034k_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034l_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034m_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-034n_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034p_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-034q_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-035a_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-035b_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-035c_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046a_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-046b_clean\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-077_clean\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-103_clean\n",
      ".... 28\n",
      "....Doing base_TAttallah2020-107_clean\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-122a_clean\n",
      ".... 31\n",
      "....Doing base_TAttallah2020-125a_clean\n",
      ".... 25\n",
      "....Doing base_TAttallah2020-128c_clean\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-130_clean\n",
      ".... 23\n",
      "....Doing base_TAttallah2020-132a_clean\n",
      ".... 26\n",
      "....Doing base_TAttallah2020-132b_clean\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-134_clean\n",
      ".... 26\n",
      "....Doing base_kc0066_01_01_1873diary_061_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_062_clean\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_063_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_064_clean\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_065_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_164_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_165_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_166_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_167_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_168_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_169_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_170_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_171_clean\n",
      ".... 13\n",
      "Total files in set all3: 298\n",
      "....Doing base_15805_01r_cropped_clean\n",
      ".... 25\n",
      "....Doing base_15805_02r_cropped_clean\n",
      ".... 25\n",
      "....Doing base_15808r_cropped_clean\n",
      ".... 29\n",
      "....Doing base_15808v_cropped_clean\n",
      ".... 16\n",
      "....Doing base_15811_02r_cropped_clean\n",
      ".... 11\n",
      "....Doing base_15811_03r_cropped_clean\n",
      ".... 24\n",
      "....Doing base_15812r_cropped_clean\n",
      ".... 21\n",
      "....Doing base_15813r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_15814_01r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15814_02r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15814_03r_cropped_clean\n",
      ".... 14\n",
      "....Doing base_15817r_cropped_clean\n",
      ".... 17\n",
      "....Doing base_15818r_cropped_clean\n",
      ".... 34\n",
      "....Doing base_15819r_cropped_clean\n",
      ".... 15\n",
      "....Doing base_15820_01r_cropped_clean\n",
      ".... 28\n",
      "Not doing line 1\n",
      "....Doing base_15820_02r_cropped_clean\n",
      ".... 31\n",
      "....Doing base_15820_03r_cropped_clean\n",
      ".... 7\n",
      "....Doing base_15820_04r_cropped_clean\n",
      ".... 32\n",
      "....Doing base_15821_01r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15821_07r_cropped_clean\n",
      ".... 21\n",
      "....Doing base_15821_08r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_15821_09r_cropped_clean\n",
      ".... 24\n",
      "....Doing base_15821_10r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_15823r_cropped_clean\n",
      ".... 11\n",
      "....Doing base_15824_01r_cropped_clean\n",
      ".... 12\n",
      "....Doing base_15827_02r_cropped_clean\n",
      ".... 13\n",
      "....Doing base_15827_03r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_15827_04r_cropped_clean\n",
      ".... 21\n",
      "....Doing base_15827_05r_cropped_clean\n",
      ".... 24\n",
      "....Doing base_15828r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15829_01r_cropped_clean\n",
      ".... 19\n",
      "....Doing base_15829_01v_cropped_clean\n",
      ".... 26\n",
      "....Doing base_15831_03r_cropped_clean\n",
      ".... 18\n",
      "....Doing base_15832_01r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15833_01r_cropped_clean\n",
      ".... 21\n",
      "....Doing base_15995_01r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_15995_02r_cropped_clean\n",
      ".... 21\n",
      "....Doing base_15996r_cropped_clean\n",
      ".... 22\n",
      "....Doing base_15997_01r_cropped_clean\n",
      ".... 6\n",
      "....Doing base_15998_02r_cropped_clean\n",
      ".... 12\n",
      "....Doing base_16000r_cropped_clean\n",
      ".... 24\n",
      "....Doing base_16002_01r_cropped_clean\n",
      ".... 19\n",
      "....Doing base_16002_02r_cropped_clean\n",
      ".... 20\n",
      "....Doing base_16005r_cropped_clean\n",
      ".... 9\n",
      "....Doing base_16012r_cropped_clean\n",
      ".... 18\n",
      "....Doing base_16013r_cropped_clean\n",
      ".... 18\n",
      "....Doing base_16014r_cropped_clean\n",
      ".... 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Doing base_16014v_cropped_clean\n",
      ".... 12\n",
      "....Doing base_16015r_cropped_clean\n",
      ".... 18\n",
      "....Doing base_16016r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_16017r_cropped_clean\n",
      ".... 12\n",
      "....Doing base_16018r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_16019r_cropped_clean\n",
      ".... 23\n",
      "....Doing base_AR53_01_025_clean\n",
      ".... 21\n",
      "....Doing base_AR53_01_026_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_027_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_029_clean\n",
      ".... 23\n",
      "....Doing base_AR53_01_031_clean\n",
      ".... 14\n",
      "....Doing base_AR53_01_033_clean\n",
      ".... 20\n",
      "....Doing base_AR53_01_034_clean\n",
      ".... 21\n",
      "....Doing base_AR53_01_035_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_036_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_037_clean\n",
      ".... 21\n",
      "....Doing base_AR53_01_038_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_039_clean\n",
      ".... 25\n",
      "....Doing base_AR53_01_040_clean\n",
      ".... 25\n",
      "....Doing base_AR55_05_006_clean\n",
      ".... 18\n",
      "....Doing base_AR55_05_007_clean\n",
      ".... 17\n",
      "....Doing base_AR56_01_046_clean\n",
      ".... 14\n",
      "....Doing base_AR56_01_047_clean\n",
      ".... 11\n",
      "....Doing base_AR56_01_048_clean\n",
      ".... 12\n",
      "....Doing base_AR56_01_049_clean\n",
      ".... 12\n",
      "....Doing base_AR56_01_052_clean\n",
      ".... 15\n",
      "....Doing base_AR56_01_053_clean\n",
      ".... 5\n",
      "....Doing base_AR56_01_054_clean\n",
      ".... 15\n",
      "....Doing base_AR56_01_055_clean\n",
      ".... 9\n",
      "....Doing base_AR56_01_058_clean\n",
      ".... 21\n",
      "....Doing base_AR56_01_059_clean\n",
      ".... 12\n",
      "....Doing base_AR56_01_060_clean\n",
      ".... 17\n",
      "....Doing base_AR56_01_061_clean\n",
      ".... 26\n",
      "....Doing base_AR56_01_062_clean\n",
      ".... 5\n",
      "....Doing base_AR56_01_064_clean\n",
      ".... 14\n",
      "....Doing base_AR56_01_065_clean\n",
      ".... 18\n",
      "....Doing base_AR56_01_066_clean\n",
      ".... 18\n",
      "....Doing base_AR56_01_067_clean\n",
      ".... 18\n",
      "....Doing base_AR56_01_077_1_clean\n",
      ".... 18\n",
      "....Doing base_AR56_01_077_2_clean\n",
      ".... 20\n",
      "....Doing base_AR56_01_078_1_clean\n",
      ".... 10\n",
      "....Doing base_AR56_01_078_2_clean\n",
      ".... 18\n",
      "....Doing base_AR56_02_002_1_clean\n",
      ".... 14\n",
      "....Doing base_AR56_02_002_2_clean\n",
      ".... 20\n",
      "....Doing base_AR56_02_003_1_clean\n",
      ".... 20\n",
      "....Doing base_AR56_02_003_2_clean\n",
      ".... 20\n",
      "....Doing base_AR56_02_013_1_clean\n",
      ".... 10\n",
      "....Doing base_AR56_02_013_2_clean\n",
      ".... 16\n",
      "....Doing base_AR56_02_014_1_clean\n",
      ".... 13\n",
      "....Doing base_AR56_02_014_2_clean\n",
      ".... 13\n",
      "....Doing base_AR56_02_016_clean\n",
      ".... 18\n",
      "....Doing base_AR56_02_017_clean\n",
      ".... 24\n",
      "....Doing base_AR56_02_018_clean\n",
      ".... 12\n",
      "done 100\n",
      "....Doing base_AR56_03_004_clean\n",
      ".... 18\n",
      "....Doing base_AR56_03_005_clean\n",
      ".... 20\n",
      "....Doing base_AR56_03_006_clean\n",
      ".... 19\n",
      "....Doing base_AR56_03_012_clean\n",
      ".... 13\n",
      "....Doing base_AR56_03_013_clean\n",
      ".... 13\n",
      "....Doing base_AR56_03_017_clean\n",
      ".... 16\n",
      "....Doing base_AR56_03_018_clean\n",
      ".... 12\n",
      "....Doing base_AR56_03_019_clean\n",
      ".... 4\n",
      "....Doing base_AR56_03_023_clean\n",
      ".... 26\n",
      "....Doing base_AR56_03_024_clean\n",
      ".... 8\n",
      "....Doing base_AR56_03_025_clean\n",
      ".... 26\n",
      "....Doing base_AR56_03_030_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_031_clean\n",
      ".... 22\n",
      "....Doing base_AR56_03_032_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_033_clean\n",
      ".... 9\n",
      "....Doing base_AR56_03_034_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_035_clean\n",
      ".... 7\n",
      "....Doing base_AR56_03_036_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_037_clean\n",
      ".... 9\n",
      "....Doing base_AR56_03_039_clean\n",
      ".... 11\n",
      "....Doing base_AR56_03_040_clean\n",
      ".... 17\n",
      "....Doing base_AR56_03_041_clean\n",
      ".... 12\n",
      "....Doing base_AR56_03_057_clean\n",
      ".... 16\n",
      "....Doing base_AR56_03_058_clean\n",
      ".... 20\n",
      "....Doing base_AR56_03_059_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_065_clean\n",
      ".... 9\n",
      "....Doing base_AR56_03_066_clean\n",
      ".... 10\n",
      "....Doing base_AR56_03_070_clean\n",
      ".... 18\n",
      "....Doing base_AR56_03_071_clean\n",
      ".... 19\n",
      "....Doing base_AR56_03_076_clean\n",
      ".... 13\n",
      "....Doing base_AR56_03_080_clean\n",
      ".... 8\n",
      "....Doing base_AR56_04_012_clean\n",
      ".... 10\n",
      "....Doing base_AR56_04_017_clean\n",
      ".... 14\n",
      "....Doing base_AR56_04_018_clean\n",
      ".... 11\n",
      "....Doing base_AR56_04_029_clean\n",
      ".... 25\n",
      "....Doing base_AR56_04_030_clean\n",
      ".... 25\n",
      "....Doing base_AR56_04_031_clean\n",
      ".... 12\n",
      "....Doing base_AR56_04_034_clean\n",
      ".... 19\n",
      "....Doing base_AR56_04_037_clean\n",
      ".... 19\n",
      "....Doing base_AR56_04_038_clean\n",
      ".... 18\n",
      "....Doing base_AR56_04_039_clean\n",
      ".... 13\n",
      "....Doing base_AR56_04_047_clean\n",
      ".... 11\n",
      "....Doing base_AR56_04_071_clean\n",
      ".... 14\n",
      "....Doing base_AR56_04_082_clean\n",
      ".... 21\n",
      "....Doing base_AR56_04_083_clean\n",
      ".... 27\n",
      "....Doing base_AR56_04_084_clean\n",
      ".... 24\n",
      "....Doing base_AR56_04_088_clean\n",
      ".... 17\n",
      "....Doing base_AR56_04_089_clean\n",
      ".... 22\n",
      "....Doing base_AR56_04_090_clean\n",
      ".... 22\n",
      "....Doing base_AR56_04_091_clean\n",
      ".... 22\n",
      "....Doing base_AR56_04_092_clean\n",
      ".... 23\n",
      "....Doing base_JPK2B_096_03_clean\n",
      ".... 26\n",
      "....Doing base_JPK2B_096_04_clean\n",
      ".... 27\n",
      "....Doing base_JPK2B_096_05_clean\n",
      ".... 14\n",
      "....Doing base_JPK2B_40_clean\n",
      ".... 28\n",
      "....Doing base_JPK2B_41_clean\n",
      ".... 14\n",
      "....Doing base_JPK2B_42_01_clean\n",
      ".... 28\n",
      "....Doing base_JPK2B_42_02_clean\n",
      ".... 29\n",
      "....Doing base_JPK2B_42_03_clean\n",
      ".... 28\n",
      "....Doing base_JPK2B_42_04_clean\n",
      ".... 20\n",
      "....Doing base_JPK2B_45_01_clean\n",
      ".... 25\n",
      "....Doing base_JPK2B_45_02_clean\n",
      ".... 10\n",
      "....Doing base_JPK2B_53_01_clean\n",
      ".... 30\n",
      "....Doing base_JPK2B_53_02_clean\n",
      ".... 5\n",
      "....Doing base_JPK2B_56_01_clean\n",
      ".... 31\n",
      "....Doing base_JPK2B_56_02_clean\n",
      ".... 12\n",
      "....Doing base_JPK2B_57_01_clean\n",
      ".... 28\n",
      "....Doing base_JPK2B_57_02_clean\n",
      ".... 14\n",
      "....Doing base_JPK2B_65_clean\n",
      ".... 25\n",
      "....Doing base_JPK2B_69_01_clean\n",
      ".... 24\n",
      "....Doing base_JPK2B_69_02_clean\n",
      ".... 26\n",
      "....Doing base_JPK2B_78_01_clean\n",
      ".... 24\n",
      "....Doing base_JPK2B_78_02_clean\n",
      ".... 25\n",
      "....Doing base_JPK2B_78_03_clean\n",
      ".... 26\n",
      "....Doing base_JPK2B_78_04_clean\n",
      ".... 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not doing line 3\n",
      "....Doing base_KJoseph18910818-001_clean\n",
      ".... 36\n",
      "....Doing base_KJoseph18910818-002_clean\n",
      ".... 12\n",
      "....Doing base_KJoseph18970910-001_clean\n",
      ".... 24\n",
      "....Doing base_KJoseph18970910-002_clean\n",
      ".... 10\n",
      "....Doing base_KJoseph18990827-001_1_clean\n",
      ".... 18\n",
      "....Doing base_KJoseph18990827-001_2_clean\n",
      ".... 19\n",
      "....Doing base_KJoseph18990827-002_1_clean\n",
      ".... 5\n",
      "....Doing base_KJoseph18990827-002_2_clean\n",
      ".... 22\n",
      "....Doing base_KJoseph18990913-001_1_clean\n",
      ".... 12\n",
      "....Doing base_KJoseph18990913-001_2_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_01v_clean\n",
      ".... 17\n",
      "....Doing base_MH_01_02r_clean\n",
      ".... 20\n",
      "....Doing base_MH_01_02v_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_03r_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_03v_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_04r_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_04v_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_05r_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_05v_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_06r_clean\n",
      ".... 20\n",
      "....Doing base_MH_01_06v_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_07r_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_07v_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_08r_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_08v_clean\n",
      ".... 19\n",
      "done 200\n",
      "....Doing base_MH_01_09r_clean\n",
      ".... 19\n",
      "....Doing base_MH_01_09v_clean\n",
      ".... 16\n",
      "....Doing base_MH_01_10r_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_10v_clean\n",
      ".... 16\n",
      "....Doing base_MH_01_11r_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_11v_clean\n",
      ".... 18\n",
      "....Doing base_MH_01_12r_clean\n",
      ".... 21\n",
      "....Doing base_MH_01_12v_clean\n",
      ".... 20\n",
      "....Doing base_MH_01_13_clean\n",
      ".... 21\n",
      "....Doing base_Nasrallah-Al001-026a_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-026b_clean\n",
      ".... 14\n",
      "....Doing base_QBat19D_004r_clean\n",
      ".... 13\n",
      "....Doing base_QBat19D_020r_clean\n",
      ".... 24\n",
      "....Doing base_QBat1L_051r_clean\n",
      ".... 19\n",
      "....Doing base_QBat1L_052r_clean\n",
      ".... 19\n",
      "....Doing base_QBat1L_053r_clean\n",
      ".... 15\n",
      "....Doing base_QBat1L_084_03r_clean\n",
      ".... 17\n",
      "....Doing base_QBat1L_087r_clean\n",
      ".... 14\n",
      "....Doing base_QBat1L_242r_clean\n",
      ".... 13\n",
      "....Doing base_QBat3D_0005r_clean\n",
      ".... 11\n",
      "....Doing base_QBat3D_0006r_clean\n",
      ".... 19\n",
      "....Doing base_QBat3D_0010r_clean\n",
      ".... 18\n",
      "....Doing base_QBat3D_1471_02r_clean\n",
      ".... 19\n",
      "....Doing base_QBat4L_003r_clean\n",
      ".... 16\n",
      "....Doing base_QBat4L_012r_clean\n",
      ".... 18\n",
      "....Doing base_QBat5H_0517r_clean\n",
      ".... 31\n",
      "....Doing base_QBat5H_0523r_clean\n",
      ".... 24\n",
      "....Doing base_QBat5L_09r_clean\n",
      ".... 19\n",
      "....Doing base_QBat7F_12r_clean\n",
      ".... 16\n",
      "....Doing base_QBat8L_02r_clean\n",
      ".... 17\n",
      "....Doing base_QBat8L_03r_clean\n",
      ".... 16\n",
      "....Doing base_TB1A_16_01_clean\n",
      ".... 25\n",
      "....Doing base_TB1A_16_02_clean\n",
      ".... 21\n",
      "....Doing base_TB1A_17_clean\n",
      ".... 21\n",
      "....Doing base_TB1A_18_clean\n",
      ".... 20\n",
      "....Doing base_TB1A_19_clean\n",
      ".... 23\n",
      "....Doing base_TB1A_25_01_clean\n",
      ".... 27\n",
      "....Doing base_YAL2A_01_01r_clean\n",
      ".... 16\n",
      "....Doing base_YAL2A_03r_clean\n",
      ".... 16\n",
      "....Doing base_YAL2A_08_02_clean\n",
      ".... 21\n",
      "....Doing base_YAL2A_11_02_clean\n",
      ".... 16\n",
      "....Doing base_YAL2A_13_02_clean\n",
      ".... 13\n",
      "....Doing base_YAL2A_14_01_clean\n",
      ".... 21\n",
      "....Doing base_YAL2A_14_03_clean\n",
      ".... 15\n",
      "....Doing base_YAL2A_16r_clean\n",
      ".... 22\n",
      "....Doing base_YAL2A_23_02r_clean\n",
      ".... 19\n",
      "....Doing base_YAL2A_24_clean\n",
      ".... 21\n",
      "....Doing base_YAL2A_40_clean\n",
      ".... 23\n",
      "....Doing base_YAL2A_47r_clean\n",
      ".... 16\n",
      "....Doing base_YAL2A_54_clean\n",
      ".... 21\n",
      "....Doing base_YAL2A_65_02_clean\n",
      ".... 18\n",
      "....Doing base_YAL2A_74r_clean\n",
      ".... 18\n",
      "....Doing base_YAL4A_01_02r_clean\n",
      ".... 15\n",
      "....Doing base_YAL4A_03_01r_clean\n",
      ".... 15\n",
      "....Doing base_YAL4A_03_01v_clean\n",
      ".... 16\n",
      "....Doing base_YAL4A_03_02r_clean\n",
      ".... 16\n",
      "....Doing base_YAL4A_03_02v_clean\n",
      ".... 16\n",
      "....Doing base_YAL4A_03_03r_clean\n",
      ".... 15\n",
      "....Doing base_YFS1A_07_clean\n",
      ".... 21\n",
      "....Doing base_YFS1A_10_clean\n",
      ".... 21\n",
      "....Doing base_YFS1A_14_clean\n",
      ".... 18\n",
      "....Doing base_YFS2A_01r_clean\n",
      ".... 26\n",
      "....Doing base_YFS2A_01v_clean\n",
      ".... 18\n",
      "....Doing base_YFS2A_02_01_1_clean\n",
      ".... 17\n",
      "....Doing base_YFS2A_02_02_2_clean\n",
      ".... 20\n",
      "....Doing base_YFS2A_04_clean\n",
      ".... 21\n",
      "....Doing base_YFS2A_05_01_clean\n",
      ".... 18\n",
      "....Doing base_YFS2A_05_02_clean\n",
      ".... 20\n",
      "....Doing base_YFS2A_05_03_clean\n",
      ".... 20\n",
      "....Doing base_YFS2A_05_04_clean\n",
      ".... 20\n",
      "....Doing base_YFS5A_09r_clean\n",
      ".... 12\n",
      "....Doing base_YFS5A_09v_clean\n",
      ".... 8\n",
      "....Doing base_YS_2C_004_clean\n",
      ".... 13\n",
      "....Doing base_YS_2C_004a_clean\n",
      ".... 11\n",
      "....Doing base_YS_2C_004b_clean\n",
      ".... 10\n",
      "....Doing base_YS_2C_004c_clean\n",
      ".... 10\n",
      "....Doing base_YS_2C_004d_clean\n",
      ".... 11\n",
      "....Doing base_YS_2C_004e_clean\n",
      ".... 11\n",
      "....Doing base_YS_2C_004f_clean\n",
      ".... 10\n",
      "....Doing base_YS_2F_015a_clean\n",
      ".... 12\n",
      "....Doing base_YS_2F_015b_clean\n",
      ".... 8\n",
      "....Doing base_YS_2F_015c_clean\n",
      ".... 13\n",
      "....Doing base_YS_2F_015d_clean\n",
      ".... 11\n",
      "....Doing base_YS_2F_015e_clean\n",
      ".... 11\n",
      "....Doing base_YS_2F_015f_clean\n",
      ".... 12\n",
      "....Doing base_YS_2F_015g_clean\n",
      ".... 11\n",
      "....Doing base_YS_2F_015i_clean\n",
      ".... 9\n",
      "....Doing base_YS_2F_015n_clean\n",
      ".... 11\n",
      "....Doing base_YS_2F_015o_clean\n",
      ".... 11\n",
      "....Doing base_YS_2F_015p_clean\n",
      ".... 12\n",
      "....Doing base_YS_2F_022a_clean\n",
      ".... 18\n",
      "....Doing base_YS_2F_022b_clean\n",
      ".... 18\n",
      "....Doing base_YS_2F_022c_clean\n",
      ".... 18\n",
      "....Doing base_YS_2F_022d_clean\n",
      ".... 16\n",
      "....Doing base_YS_2F_022e_clean\n",
      ".... 17\n",
      "....Doing base_YS_2F_022f_clean\n",
      ".... 11\n",
      "....Doing base_kc0066_01_01_1873diary_077_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_078_clean\n",
      ".... 14\n",
      "Total files in set all4: 149\n",
      "....Doing base_2015 5-03 El-Khouri_Letter to Jennie Jabaley from Lebanon Oct18 1960_1_clean\n",
      ".... 20\n",
      "....Doing base_2015 5-07 El-Khouri_Letter to Joseph from Lebanon Dec17 1959_1_clean\n",
      ".... 21\n",
      "Not doing line 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/torch/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not doing line 2\n",
      "....Doing base_2015 5-07b El-Khouri_Letter to Joseph from Lebanon Dec17 1959_2_1_clean\n",
      ".... 16\n",
      "....Doing base_2015 5-07b El-Khouri_Letter to Joseph from Lebanon Dec17 1959_2_2_clean\n",
      ".... 13\n",
      "....Doing base_2015 5-08b El-Khouri_Letter to Joseph from Lebanon Dec19 1957_2_1_clean\n",
      ".... 19\n",
      "....Doing base_2015 5-08b El-Khouri_Letter to Joseph from Lebanon Dec19 1957_2_2_clean\n",
      ".... 20\n",
      "....Doing base_2015 5-08c El-Khouri_Letter to Joseph from Lebanon Dec19 1957_3_clean\n",
      ".... 18\n",
      "....Doing base_2015 5-09b El-Khouri_Letter to Joseph from Lebanon Dec21 1960_2_clean\n",
      ".... 20\n",
      "....Doing base_2015 5-13 El-Khouri_Letter to Joseph from Lebanon Feb29 1960 1_clean\n",
      ".... 18\n",
      "....Doing base_2015 5-13b El-Khouri_Letter to Joseph from Lebanon Feb29 1960_2_1_clean\n",
      ".... 19\n",
      "....Doing base_AR53_01_006_clean\n",
      ".... 19\n",
      "....Doing base_AR53_01_008_clean\n",
      ".... 21\n",
      "....Doing base_AR53_01_009_clean\n",
      ".... 24\n",
      "....Doing base_AR53_01_010_clean\n",
      ".... 22\n",
      "....Doing base_AR53_01_011_clean\n",
      ".... 21\n",
      "....Doing base_AR56_02_006_clean\n",
      ".... 18\n",
      "....Doing base_AR56_03_002_clean\n",
      ".... 12\n",
      "....Doing base_AR56_03_003_clean\n",
      ".... 15\n",
      "....Doing base_AR56_03_007_clean\n",
      ".... 18\n",
      "....Doing base_AR56_03_008_clean\n",
      ".... 17\n",
      "....Doing base_AR56_04_033_clean\n",
      ".... 14\n",
      "....Doing base_AR56_04_035_clean\n",
      ".... 18\n",
      "....Doing base_AR56_04_036_clean\n",
      ".... 21\n",
      "....Doing base_KEllis2018-139a_clean\n",
      ".... 22\n",
      "....Doing base_KEllis2018-139b_clean\n",
      ".... 22\n",
      "....Doing base_KEllis2018-139c_clean\n",
      ".... 21\n",
      "....Doing base_KEllis2018-139d_clean\n",
      ".... 24\n",
      "....Doing base_KEllis2018-139e_clean\n",
      ".... 24\n",
      "....Doing base_KEllis2018-147e_clean\n",
      ".... 18\n",
      "....Doing base_KEllis2018-147f_clean\n",
      ".... 16\n",
      "....Doing base_KEllis2018-166a_clean\n",
      ".... 24\n",
      "....Doing base_KEllis2018-166b_clean\n",
      ".... 25\n",
      "....Doing base_KEllis2018-166c_clean\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169a_1_clean\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169a_2_clean\n",
      ".... 15\n",
      "....Doing base_KEllis2018-169b_1_clean\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169b_2_clean\n",
      ".... 17\n",
      "....Doing base_KEllis2018-169c_1_clean\n",
      ".... 14\n",
      "....Doing base_KEllis2018-169c_2_clean\n",
      ".... 14\n",
      "....Doing base_KEllis2018-169d_1_clean\n",
      ".... 16\n",
      "....Doing base_KEllis2018-169d_2_clean\n",
      ".... 16\n",
      "....Doing base_KEllis2018-175a_clean\n",
      ".... 18\n",
      "....Doing base_KEllis2018-175b_clean\n",
      ".... 17\n",
      "....Doing base_KEllis2018-193a_clean\n",
      ".... 20\n",
      "....Doing base_KEllis2018-193b_clean\n",
      ".... 21\n",
      "....Doing base_KEllis2018-193c_clean\n",
      ".... 16\n",
      "....Doing base_KEllis2019-105a_clean\n",
      ".... 10\n",
      "....Doing base_KEllis2019-106a_clean\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-010a_clean\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-010b_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010c_clean\n",
      ".... 13\n",
      "....Doing base_Nasrallah-Al001-010d_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010e_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010f_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010g_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010h_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010i_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010j_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010k_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010l_clean\n",
      ".... 11\n",
      "....Doing base_Nasrallah-Al001-010m_clean\n",
      ".... 12\n",
      "....Doing base_Nasrallah-Al001-010n_clean\n",
      ".... 10\n",
      "....Doing base_Nasrallah-Al001-011a_clean\n",
      ".... 11\n",
      "....Doing base_Nasrallah-Al001-013b_clean\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-013c_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013d_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013e_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013f_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-013g_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014b_clean\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014c_clean\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014d_clean\n",
      ".... 14\n",
      "....Doing base_Nasrallah-Al001-014e_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014f_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014g_clean\n",
      ".... 15\n",
      "....Doing base_Nasrallah-Al001-014h_clean\n",
      ".... 14\n",
      "....Doing base_TAttallah2020-019_clean\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-020_clean\n",
      ".... 15\n",
      "....Doing base_TAttallah2020-023_clean\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-024_clean\n",
      ".... 21\n",
      "....Doing base_TAttallah2020-026_clean\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-027a_clean\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-027b_clean\n",
      ".... 18\n",
      "....Doing base_TAttallah2020-029_clean\n",
      ".... 13\n",
      "....Doing base_TAttallah2020-031a_clean\n",
      ".... 10\n",
      "....Doing base_TAttallah2020-033a_clean\n",
      ".... 20\n",
      "....Doing base_TAttallah2020-034_clean\n",
      ".... 25\n",
      "....Doing base_TAttallah2020-035_clean\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-037_clean\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-038_clean\n",
      ".... 12\n",
      "....Doing base_TAttallah2020-039_clean\n",
      ".... 17\n",
      "....Doing base_TAttallah2020-094a_resize_clean\n",
      ".... 32\n",
      "....Doing base_TAttallah2020-095_resize_clean\n",
      ".... 24\n",
      "....Doing base_TAttallah2020-099a_resize_clean\n",
      ".... 29\n",
      "....Doing base_TAttallah2020-105_resize_clean\n",
      ".... 29\n",
      "....Doing base_TAttallah2020-107_resize_clean\n",
      ".... 27\n",
      "....Doing base_kc0066_01_01_1873diary_005_clean\n",
      ".... 12\n",
      "....Doing base_kc0066_01_01_1873diary_006_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_007_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_008_clean\n",
      ".... 14\n",
      "done 100\n",
      "....Doing base_kc0066_01_01_1873diary_009_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_010_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_011_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_012_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_013_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_014_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_015_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_016_clean\n",
      ".... 9\n",
      "....Doing base_kc0066_01_01_1873diary_017_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_018_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_019_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_020_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_021_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_022_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_025_clean\n",
      ".... 11\n",
      "....Doing base_kc0066_01_01_1873diary_026_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_027_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_028_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_029_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_030_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_031_clean\n",
      ".... 14\n",
      "Not doing line 1\n",
      "....Doing base_kc0066_01_01_1873diary_032_clean\n",
      ".... 9\n",
      "....Doing base_kc0066_01_01_1873diary_033_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_034_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_035_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_036_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_037_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_038_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_039_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_040_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_041_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_042_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_043_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_044_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_045_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_046_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_047_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_048_clean\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_049_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_050_clean\n",
      ".... 16\n",
      "....Doing base_kc0066_01_01_1873diary_051_clean\n",
      ".... 12\n",
      "....Doing base_kc0066_01_01_1873diary_052_clean\n",
      ".... 14\n",
      "....Doing base_kc0066_01_01_1873diary_053_clean\n",
      ".... 11\n",
      "....Doing base_kc0066_01_01_1873diary_054_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_055_clean\n",
      ".... 13\n",
      "....Doing base_kc0066_01_01_1873diary_056_clean\n",
      ".... 10\n",
      "....Doing base_kc0066_01_01_1873diary_057_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_058_clean\n",
      ".... 15\n",
      "....Doing base_kc0066_01_01_1873diary_059_clean\n",
      ".... 15\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# This routine is for scribe arabic\n",
    "def preprocess_scribe():\n",
    "    # Make sure there is a / at end of path\n",
    "    paths = ['/home/msaeed3/mehreen/datasets/scribeArabicClean/jsons/']\n",
    "    set_name = [#'all0', \n",
    "                'all1', 'all2', 'all3', 'all4']\n",
    "    input_files = [#'/home/msaeed3/mehreen/datasets/scribeArabicClean/images/df0.csv', \n",
    "                  '/home/msaeed3/mehreen/datasets/scribeArabicClean/images/df1.csv',\n",
    "                  '/home/msaeed3/mehreen/datasets/scribeArabicClean/images/df2.csv',\n",
    "                  '/home/msaeed3/mehreen/datasets/scribeArabicClean/images/df3.csv',\n",
    "                  '/home/msaeed3/mehreen/datasets/scribeArabicClean/images/df4.csv' ]\n",
    "    \n",
    "    for set_to_use, input_file in zip(set_name, input_files):\n",
    "        train_gt = process_notdone_arabic_dir(paths[0], set_to_use, ['paragraph', 'text'], \n",
    "                                      input_file=input_file)\n",
    "     \n",
    "        json_file = os.path.join(paths[0], set_to_use + '.json')\n",
    "    \n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(train_gt, f)\n",
    "            \n",
    "preprocess_scribe()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4dba9",
   "metadata": {},
   "source": [
    "# Uncomment this cell to check for a single file\n",
    "\n",
    "out_path = '/home/msaeed3/mehreen/datasets/MoiseK/'\n",
    "\n",
    "check_file = '15805_01r_cropped.jpg' \n",
    "set_name = 'all_05'\n",
    "csv_file = '/home/msaeed3/mehreen/datasets/scribeArabic/batch_05.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.baseline = convert_str_to_int_tuples(df.baseline)\n",
    "\n",
    "df.polygon_pts = convert_str_to_int_tuples(df.polygon_pts)\n",
    "\n",
    "files = df.image_file\n",
    "\n",
    "all_ground_truth = []\n",
    "for img_path in set(files):\n",
    "    \n",
    "    if check_file in img_path:\n",
    "        print('***')\n",
    "        para_df = df[df.image_file == img_path]\n",
    "        para_df = para_df.copy()\n",
    "        para_df = para_df.reset_index(drop=True)\n",
    "        _, image_width = get_img_dim(para_df.image_file[0])\n",
    "        # To have a left to right reading order\n",
    "        para_df.baseline = flip_x_coord(para_df.baseline, image_width, reverse=False)\n",
    "        para_df.polygon_pts = flip_x_coord(para_df.polygon_pts, image_width) \n",
    "        \n",
    "        valid_region = any(s in para_df['region_type'][0] for s in ['paragraph','text'])\n",
    "        if not valid_region:\n",
    "            print('NOT DONE')\n",
    "        json_path = handle_single_para(para_df, out_path+set_name, flip=True)       \n",
    "        print(json_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16415cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpara_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'para_df' is not defined"
     ]
    }
   ],
   "source": [
    "para_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19efa",
   "metadata": {},
   "source": [
    "# This is for priinting the json file in readable format\n",
    "\n",
    "json_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train.json'\n",
    "with open(json_file, 'r') as fin:\n",
    "    json_obj = json.load(fin)\n",
    "\n",
    "out_file = '/home/msaeed3/mehreen/datasets/arabic_all/Train_readable.json'\n",
    "with open(out_file, 'w') as fout:\n",
    "    json.dump(json_obj, fout, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
