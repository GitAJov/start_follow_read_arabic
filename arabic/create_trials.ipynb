{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397182f2",
   "metadata": {},
   "source": [
    "# Create different splits of train, valid, test sets\n",
    "# Is hard coded to split the public dataset with 1100 training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbb976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import shutil\n",
    "import create_set_routines as create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d815cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.load(f, Loader=yaml.loader.SafeLoader)\n",
    "    return config       \n",
    "\n",
    "\n",
    "def write_json_files(directory, json_sets, total_train_valid):\n",
    "    setnames = ['train_{}.json'.format(total_train_valid), \n",
    "                'valid_{}.json'.format(total_train_valid),\n",
    "                'test_{}.json'.format(total_train_valid)]\n",
    "    \n",
    "    \n",
    "    for s, json_obj in zip(setnames, json_sets):\n",
    "        json_filename = os.path.join(directory, s)\n",
    "        with open(json_filename, 'w') as fout:\n",
    "            json_str = json.dumps(json_obj)\n",
    "            fout.write(json_str)\n",
    "            \n",
    "def get_char_set_length(char_set_path):\n",
    "    with open(char_set_path) as f:\n",
    "        char_set = json.load(f)\n",
    "    return len(char_set['idx_to_char'])\n",
    "\n",
    "# Pretraining to start from init folder. Fine tuned network to go into pretrain folder    \n",
    "def edit_pretrain_entries(config, set_dir, stop_after_no_improvement=15, images_per_epoch=100, PRETRAIN_SUFFIX=\"\", \n",
    "                          lf_images_per_epoch=0):\n",
    "    if lf_images_per_epoch == 0:\n",
    "        lf_images_per_epoch = images_per_epoch\n",
    "\n",
    "    # HW specific\n",
    "    config['network']['hw']['char_set_path'] = '../trials/charset.json'\n",
    "    config['network']['hw']['use_instance_norm'] = True\n",
    "    config['network']['hw']['transfer_path'] = '../trials/pretrained/hw.pt'\n",
    "    config['network']['hw']['num_of_outputs'] = get_char_set_length(config['network']['hw']['char_set_path']) + 1\n",
    "    config['network']['hw']['input_height'] = 60\n",
    "\n",
    "    # GEneral\n",
    "    config['pretraining']['hw']['stop_after_no_improvement'] = stop_after_no_improvement\n",
    "    config['pretraining']['sol']['stop_after_no_improvement'] = stop_after_no_improvement\n",
    "    config['pretraining']['lf']['stop_after_no_improvement'] = stop_after_no_improvement\n",
    "    \n",
    "    config['pretraining']['lf']['images_per_epoch'] = lf_images_per_epoch\n",
    "    config['pretraining']['sol']['images_per_epoch'] = images_per_epoch\n",
    "    config['pretraining']['hw']['images_per_epoch'] = images_per_epoch\n",
    "    \n",
    "    config['pretraining']['lf']['log_file'] = os.path.join(set_dir, 'pretrain_lf_log.csv')\n",
    "    config['pretraining']['sol']['log_file'] = os.path.join(set_dir, 'pretrain_sol_log.csv')\n",
    "    config['pretraining']['hw']['log_file'] = os.path.join(set_dir, 'pretrain_hw_log.csv')\n",
    "    \n",
    "    config['pretraining']['snapshot_path'] = config['training']['snapshot']['pretrain']\n",
    "    config['pretraining']['pretrained_path'] = config['training']['snapshot']['init']\n",
    "    \n",
    "    config['pretraining']['training_set']['file_list'] = os.path.join(set_dir, 'pretrain_train_{}.json'.format(PRETRAIN_SUFFIX))\n",
    "    config['pretraining']['validation_set']['file_list'] = os.path.join(set_dir, 'pretrain_valid_{}.json'.format(PRETRAIN_SUFFIX))\n",
    "    \n",
    "    return config\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b4915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d906d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a22364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Split the entire json randomly (seed as input param) to train, valid, test sets\n",
    "# Total train valid is array with [total_train_examples total_valid_examples]\n",
    "# Also writes division file\n",
    "def split_pretrain_json_and_write(all_json_files, train_valid, seed, set_dir):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    all_json = all_json_files.copy()\n",
    "    shuffled_ind = np.random.permutation(len(all_json))\n",
    "    all_json = [all_json_files[i] for i in shuffled_ind]\n",
    "    all_json_count = len(all_json)\n",
    "    \n",
    "    total_train = train_valid[0]\n",
    "\n",
    "    total_valid = train_valid[1]\n",
    "    total_test = all_json_count - total_train - total_valid\n",
    "\n",
    "    train_json = all_json[:total_train]\n",
    "    valid_json = all_json[total_train:total_valid+total_train]\n",
    "    test_json = all_json[total_valid+total_train:]\n",
    "    total_train_valid = total_train + total_valid\n",
    "    \n",
    "        # Write files\n",
    "    train_file = os.path.join(set_dir, 'pretrain_train_{}.json'.format(total_train_valid))\n",
    "    valid_file = os.path.join(set_dir, 'pretrain_valid_{}.json'.format(total_train_valid))\n",
    "    test_file = os.path.join(set_dir, 'pretrain_test_{}.json'.format(total_train_valid))\n",
    "    \n",
    "    for json_obj, file in zip([train_json, valid_json, test_json],\n",
    "                              [train_file, valid_file, test_file]):\n",
    "        with open(file, 'w') as fout:\n",
    "            json.dump(json_obj, fout)\n",
    "    \n",
    "    division_file = set_dir + 'division_{}.txt'.format(total_train_valid)\n",
    "    \n",
    "    with open(division_file, 'w') as f:\n",
    "        f.write(str(list(shuffled_ind)))\n",
    "        f.write('\\nSEED is {}\\n'.format(seed))\n",
    "        img_files = [(ind, f[1]) for ind, f in enumerate(all_json)]\n",
    "        f.write(str(img_files))\n",
    "    \n",
    "\n",
    "def write_config_for_pretrain_only(config, set_dir, total_train_valid, copy_pretrained=False):\n",
    "    config_filename = 'config'\n",
    "    # Test file    \n",
    "    config['testing'] = dict()\n",
    "    config['testing']['test_file'] = os.path.join(set_dir, 'pretrain_test_{}.json'.format(total_train_valid))\n",
    "    \n",
    "        \n",
    "    # Reset interval\n",
    "    config['training']['lf']['reset_interval'] = 60*60*120  # 5 days\n",
    "    config['training']['hw']['reset_interval'] = 60*60*120\n",
    "    config['training']['sol']['reset_interval'] = 60*60*120\n",
    "    \n",
    "    # Stage 2 specific\n",
    "    config['training']['alignment']['train_log_file'] = set_dir + f'log_align_train_{total_train_valid}.csv'\n",
    "    config['training']['alignment']['validate_log_file'] = set_dir + f'log_align_validate_{total_train_valid}.csv'\n",
    "\n",
    "    config['training']['hw']['log_file'] = set_dir + f'log_hw_{total_train_valid}.csv'\n",
    "    config['training']['lf']['log_file'] = set_dir + f'log_lf_{total_train_valid}.csv'\n",
    "    config['training']['sol']['log_file'] = set_dir + f'log_sol_{total_train_valid}.csv'\n",
    "\n",
    "    config['training']['snapshot']['best_overall'] = set_dir + f'snapshot_{total_train_valid}/best_overall/'\n",
    "    config['training']['snapshot']['best_validation'] = set_dir + f'snapshot_{total_train_valid}/best_validation/'\n",
    "    config['training']['snapshot']['current'] = set_dir + f'snapshot_{total_train_valid}/current/'\n",
    "\n",
    "    config['training']['training_set']['file_list'] = set_dir + f'train_{total_train_valid}'\n",
    "    config['training']['validation_set']['file_list'] = set_dir + f'valid_{total_train_valid}'\n",
    "    snapshot_folder = set_dir+'snapshot_{}/'.format(total_train_valid)\n",
    "    if not os.path.exists(snapshot_folder):\n",
    "        os.mkdir(snapshot_folder)\n",
    "    # Individual networks to start training from \n",
    "    # Copy the networks in the respective current folder\n",
    "    \n",
    "    for folder in ['pretrain/', 'init/']:\n",
    "        dst_folder = set_dir+'snapshot_{}/{}/'.format(total_train_valid, folder)\n",
    "        config['training']['snapshot'][folder[:-1]] = os.path.join(set_dir, \n",
    "                                                                  'snapshot_{}/'.format(total_train_valid),\n",
    "                                                                        folder)\n",
    "    config = edit_pretrain_entries(config, set_dir, \n",
    "                                   stop_after_no_improvement=40, images_per_epoch=2000,\n",
    "                                   PRETRAIN_SUFFIX=total_train_valid, lf_images_per_epoch=400)\n",
    "    config_filename = os.path.join(set_dir, \n",
    "                                   config_filename + '_{}.yaml'.format(total_train_valid))\n",
    "    with open(config_filename, 'w') as fout:\n",
    "        yaml.dump(config, fout) \n",
    "\n",
    "\n",
    "def main_create_only_pretraining_sets():\n",
    "\n",
    "    # This will create a reproducible split of train, valid, test sets            \n",
    "    SEED = 37\n",
    "    main_data_sfr_dir = '../data_files/sfr_arabic'\n",
    "            \n",
    "    input_pretrain_batch = ['public_sfr']\n",
    "    # Will run on public dataset with 1100 training images\n",
    "    output_dir = '../trials/public_1100/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    \n",
    "    sets_todo = ['set0/', 'set1/', 'set2/']\n",
    "\n",
    "    train_valid = [1100, 50]\n",
    "    total_train_valid = train_valid[0] + train_valid[1]\n",
    "    PRETRAIN_SUFFIX = total_train_valid\n",
    "    sample_config = get_config('../trials/sample_config.yaml')  \n",
    "\n",
    "    for index, _ in enumerate(sets_todo):\n",
    "        SEED = SEED+index*10\n",
    "        set_dir = output_dir + sets_todo[index]\n",
    "        # Create set dir\n",
    "        if not os.path.exists(set_dir):\n",
    "            os.mkdir(set_dir)   \n",
    "        # Get json for all files\n",
    "        all_pretrain_json = create.get_all_data(main_data_sfr_dir, input_pretrain_batch)\n",
    "        # Split json to train, test, valid and write them all. Also writes division file\n",
    "        split_pretrain_json_and_write(all_pretrain_json, train_valid, SEED, set_dir)\n",
    "        write_config_for_pretrain_only(sample_config, set_dir, total_train_valid, copy_pretrained=False)\n",
    "        \n",
    "        \n",
    "    print('done') \n",
    "            \n",
    "    \n",
    "\n",
    "# To create sets with different train/valid/test images for stage1 training   \n",
    "main_create_only_pretraining_sets()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec86a2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff6292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4f54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaab0fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1644"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how to retreive all JSON data\n",
    "main_data_sfr_dir = '../data_files/sfr_arabic'\n",
    "input_pretrain_batch = ['public_sfr', 'restricted_sfr']\n",
    "all_json = create.get_all_data(main_data_sfr_dir, input_pretrain_batch)\n",
    "print(len(all_json))\n",
    "# Trial 13: 1414 images\n",
    "s2 = set([os.path.split(img)[1] for [j,img] in all_json])\n",
    "len(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e37b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n"
     ]
    }
   ],
   "source": [
    "# This is how to retreive all JSON data\n",
    "main_data_sfr_dir = '/home/msaeed3/mehreen/datasets/MoiseK/datasets_sfr_ver2/'\n",
    "input_pretrain_batch = ['pretrain_01', 'pretrain_02', 'pretrain_03', 'pretrain_04', 'pretrain_05', 'pretrain_06', \n",
    "                        'pretrain_07', 'pretrain_08']\n",
    "all_pretrain_json = create.get_all_data(main_data_sfr_dir, input_pretrain_batch)\n",
    "print(len(all_pretrain_json))\n",
    "# Trial 13: 1414 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfcd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = set([os.path.split(img)[1] for [j,img] in all_pretrain_json])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934cf849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Oussani2018-0286_2.jpg', 'kc0061_01_04_19210504_001a.jpg'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.difference(s2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
